# pyright: reportCallIssue=false, reportUnboundVariable=false, reportAttributeAccessIssue=false
import torch
import torch.nn as nn
import logging
from typing import Tuple, Union, Dict, Any, Optional, cast
import time
import copy
import torch.jit

# ÂÆö‰πâ‰∏Ä‰∏™ÈÄöÁî®Ê®°ÂûãÁ±ªÂûãÔºåÂèØ‰ª•ÊòØÊ†áÂáÜModuleÊàñJITÊ®°Âûã
ModelType = Union[nn.Module, torch.jit.ScriptModule, Any]

def count_parameters(model: ModelType) -> int:
    """
    Count the number of trainable parameters in a model
    
    Args:
        model: PyTorch model
        
    Returns:
        int: Number of trainable parameters
    """
    try:
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    except (AttributeError, RuntimeError) as e:
        logging.warning(f"‚ö†Ô∏è Unable to count parameters normally: {e}")
        try:
            # Â∞ùËØï‰∏çÊ£ÄÊü•requires_grad
            return sum(p.numel() for p in model.parameters())
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Unable to count parameters in model, returning 0: {e}")
            return 0

def get_kernel_size(kernel_size) -> Tuple[int, int]:
    """
    Safely get the kernel size as a 2-tuple
    
    Args:
        kernel_size: Kernel size (int or tuple)
        
    Returns:
        Tuple[int, int]: Kernel size as a 2-tuple
    """
    if isinstance(kernel_size, tuple):
        if len(kernel_size) == 2:
            return kernel_size
        elif len(kernel_size) == 1:
            return (kernel_size[0], kernel_size[0])
    elif isinstance(kernel_size, int):
        return (kernel_size, kernel_size)
    return (1, 1)  # Default value

def calculate_flops(model, input_size=(1, 3, 224, 224)):
    """
    ËÆ°ÁÆóÊ®°ÂûãÁöÑFLOPsÔºàÊµÆÁÇπËøêÁÆóÊ¨°Êï∞Ôºâ
    
    Args:
        model: PyTorchÊ®°Âûã
        input_size: ËæìÂÖ•Âº†ÈáèÁöÑÂ∞∫ÂØ∏ (B, C, H, W)
        
    Returns:
        float: Ê®°ÂûãÁöÑFLOPs
    """
    import torch
    import torch.nn as nn
    
    def conv_flops(module, input_size, output_size):
        # ËÆ°ÁÆóÂç∑ÁßØÂ±ÇÁöÑFLOPs
        if not isinstance(module, nn.Conv2d):
            return 0
            
        # Â¶ÇÊûúÊòØÊï¥‰∏™ÈÄöÈÅì‰∏∫Èõ∂ÁöÑÂâ™ÊûùÂç∑ÁßØÂ±ÇÔºåËøîÂõûÈõ∂FLOPs
        in_channels = module.in_channels
        out_channels = module.out_channels
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÈõ∂ÈÄöÈÅìÔºàÊï¥‰∏™ËæìÂá∫ÈÄöÈÅìÊùÉÈáç‰∏∫Èõ∂Ôºâ
        effective_out_channels = out_channels
        for i in range(out_channels):
            if torch.sum(torch.abs(module.weight[i])) == 0:
                effective_out_channels -= 1  # ÂáèÂ∞ëÊúâÊïàËæìÂá∫ÈÄöÈÅìÊï∞
        
        # Â¶ÇÊûúÊâÄÊúâÈÄöÈÅìÈÉΩË¢´Ââ™ÊûùÔºåËøîÂõû0 FLOPs
        if effective_out_channels == 0:
            return 0
        
        # ËÆ°ÁÆóÂç∑ÁßØÂ±ÇÁöÑFLOPs
        batch_size = input_size[0]
        output_h, output_w = output_size[2], output_size[3]
        
        kernel_h, kernel_w = module.kernel_size if isinstance(module.kernel_size, tuple) else (module.kernel_size, module.kernel_size)
        groups = module.groups
        
        # Âç∑ÁßØÂ±ÇÁöÑFLOPsËÆ°ÁÆóÂÖ¨Âºè: 2 * B * H_out * W_out * C_in * C_out * K_h * K_w / groups
        flops = 2 * batch_size * output_h * output_w * in_channels * effective_out_channels * kernel_h * kernel_w / groups
        
        # Â¶ÇÊûúÊúâbiasÔºåÂä†‰∏äbiasÁöÑFLOPs: B * H_out * W_out * C_out
        if module.bias is not None:
            flops += batch_size * output_h * output_w * effective_out_channels
            
        return flops
    
    def linear_flops(module, input_size, output_size):
        # ËÆ°ÁÆóÂÖ®ËøûÊé•Â±ÇÁöÑFLOPs
        if not isinstance(module, nn.Linear):
            return 0
            
        batch_size = input_size[0]
        in_features = module.in_features
        out_features = module.out_features
        
        # Á∫øÊÄßÂ±ÇÁöÑFLOPsËÆ°ÁÆóÂÖ¨Âºè: 2 * B * in_features * out_features
        flops = 2 * batch_size * in_features * out_features
        
        # Â¶ÇÊûúÊúâbiasÔºåÂä†‰∏äbiasÁöÑFLOPs: B * out_features
        if module.bias is not None:
            flops += batch_size * out_features
            
        return flops
    
    def bn_flops(module, input_size):
        # ËÆ°ÁÆóÊâπÈáèÂΩí‰∏ÄÂåñÂ±ÇÁöÑFLOPs
        if not isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            return 0
            
        batch_size = input_size[0]
        num_features = module.num_features
        
        # ËÆ°ÁÆóÁâπÂæÅÂÖÉÁ¥†ÊÄªÊï∞
        num_elements = batch_size * num_features
        if len(input_size) > 2:
            for dim in input_size[2:]:
                num_elements *= dim
        
        # BNÂ±ÇÁöÑFLOPsËÆ°ÁÆóÂÖ¨Âºè: 2 * num_elements (‰∏ÄÊ¨°ÂáèÂùáÂÄºÔºå‰∏ÄÊ¨°Èô§‰ª•Ê†áÂáÜÂ∑Æ)
        flops = 2 * num_elements
        
        # Â¶ÇÊûúÊúâaffineÂèòÊç¢ÔºåÂä†‰∏äscaleÂíåshiftÁöÑFLOPs: 2 * num_elements
        if module.affine:
            flops += 2 * num_elements
            
        return flops
    
    # Ë∑üË∏™ÊØèÂ±ÇÁöÑËæìÂÖ•ÂíåËæìÂá∫Â§ßÂ∞è
    def register_hooks(model):
        model_hooks = []
        model_flops = [0]  # ‰ΩøÁî®ÂàóË°®Â≠òÂÇ®Ôºå‰ª•‰æøÂú®Èó≠ÂåÖ‰∏≠‰øÆÊîπ
        
        def hook_fn(module, input, output):
            # Á°Æ‰øùËæìÂÖ•ÊòØÂÖÉÁªÑ
            if not input:
                return
            if not isinstance(input, tuple):
                input = (input,)
                
            input_size = input[0].size()
            output_size = output.size() if isinstance(output, torch.Tensor) else output[0].size()
            
            # ËÆ°ÁÆóÂΩìÂâçÂ±ÇÁöÑFLOPs
            if isinstance(module, nn.Conv2d):
                flops = conv_flops(module, input_size, output_size)
            elif isinstance(module, nn.Linear):
                flops = linear_flops(module, input_size, output_size)
            elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                flops = bn_flops(module, input_size)
            else:
                flops = 0  # ÂÖ∂‰ªñÂ±ÇÊöÇ‰∏çËÆ°ÁÆó
                
            model_flops[0] += flops
        
        # ‰∏∫ÊØè‰∏™Â≠êÊ®°ÂùóÊ≥®ÂÜåÈí©Â≠ê
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                hook = module.register_forward_hook(hook_fn)
                model_hooks.append(hook)
                
        return model_hooks, model_flops
    
    # ÂàõÂª∫ÈöèÊú∫ËæìÂÖ•
    device = next(model.parameters()).device
    x = torch.randn(input_size).to(device)
    
    # Ê≥®ÂÜåÈí©Â≠ê
    hooks, flops = register_hooks(model)
    
    # ÊâßË°åÂâçÂêë‰º†Êí≠
    model.eval()
    with torch.no_grad():
        _ = model(x)
        
    # ÁßªÈô§Èí©Â≠ê
    for hook in hooks:
        hook.remove()
        
    # ËøîÂõûÊÄªFLOPs
    return flops[0]

def compare_flops(original_model, pruned_model, input_size=(1, 3, 32, 32)) -> Dict[str, Any]:
    """
    Compare FLOPs between original and pruned models
    
    Args:
        original_model: Original model
        pruned_model: Pruned model
        input_size: Input tensor size
        
    Returns:
        Dict with original_flops, pruned_flops, reduction_percentage
    """
    logging.info("üìä Calculating original model FLOPs...")
    original_flops = calculate_flops(original_model, input_size)
    
    logging.info("üìä Calculating pruned model FLOPs...")
    pruned_flops = calculate_flops(pruned_model, input_size)
    
    # Calculate FLOPs reduction
    if original_flops > 0:
        reduction_percentage = 100.0 * (1.0 - pruned_flops / original_flops)
        logging.info(f"üìä FLOPs change: {original_flops/1e6:.2f}M -> {pruned_flops/1e6:.2f}M")
        logging.info(f"üìä FLOPs reduction: {reduction_percentage:.2f}%")
    else:
        reduction_percentage = 0.0
        logging.warning("‚ö†Ô∏è Cannot calculate FLOPs reduction: original FLOPs is 0")
    
    # Calculate parameter counts
    original_params = count_parameters(original_model)
    pruned_params = count_parameters(pruned_model)
    
    if original_params > 0:
        param_reduction = 100.0 * (1.0 - pruned_params / original_params)
        logging.info(f"üìä Parameters change: {original_params} -> {pruned_params}")
        logging.info(f"üìä Parameters reduction: {param_reduction:.2f}%")
    else:
        param_reduction = 0.0
    
    return {
        'original_flops': original_flops,
        'pruned_flops': pruned_flops,
        'flops_reduction': reduction_percentage,
        'original_params': original_params,
        'pruned_params': pruned_params,
        'param_reduction': param_reduction
    }

def measure_inference_time(model, input_size=(1, 3, 32, 32), num_iterations=100, device=None, repeat_count=5, use_jit=False):
    """
    Measure the average inference time of a model.
    
    Args:
        model: PyTorch model
        input_size: Input tensor size (batch_size, channels, height, width)
        num_iterations: Number of iterations for each measurement round
        device: Device to run inference on
        repeat_count: Number of measurement rounds to perform (taking median)
        use_jit: Whether to use PyTorch JIT compilation to optimize inference
        
    Returns:
        float: Average inference time in milliseconds
    """
    if model is None:
        logging.error("‚ùå Model is None, cannot measure inference time")
        return 0
    
    # Set model to evaluation mode
    model.eval()
    
    # Get device
    if device is None:
        try:
            device = next(model.parameters()).device
        except StopIteration:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Create random input with the right shape for the model
    dummy_input = torch.randn(input_size, device=device)
    
    # Apply JIT optimization if requested
    if use_jit:
        try:
            logging.info("üöÄ Optimizing model with PyTorch JIT compilation...")
            # Try scripting the model for better performance
            with torch.no_grad():
                # Make a deep copy of the model to avoid modifying the original
                model_copy = copy.deepcopy(model)
                
                # Use torch.jit.trace for model optimization
                optimized_model = torch.jit.trace(model_copy, dummy_input)
                
                # Verify the JIT model works correctly
                orig_output = model(dummy_input)
                jit_output = optimized_model(dummy_input)
                
                # Check that outputs are similar
                if torch.allclose(orig_output, jit_output, rtol=1e-3, atol=1e-4):
                    logging.info("‚úÖ JIT optimization successful, using optimized model")
                    model = optimized_model
                else:
                    logging.warning("‚ö†Ô∏è JIT model output differs from original model, falling back to non-JIT version")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è JIT optimization failed, using original model: {e}")
    
    # Adjust iterations based on device
    if device.type == 'cpu':
        # Use fewer iterations on CPU as it's slower
        num_iterations = max(20, num_iterations // 5)
        warmup_iterations = 20
    else:
        warmup_iterations = 50
    
    # Thread priority boost (try to minimize OS interruptions)
    try:
        import os
        os.nice(-10)  # Increase process priority if possible
    except (ImportError, PermissionError):
        pass
    
    # Multiple measurement rounds
    all_times = []
    
    for round_idx in range(repeat_count):
        # Extensive warmup phase
        with torch.no_grad():
            for _ in range(warmup_iterations):
                _ = model(dummy_input)
        
        # Synchronize CUDA operations before measurement
        if device.type == 'cuda':
            torch.cuda.synchronize()
            # Clear GPU cache
            torch.cuda.empty_cache()
        
        # Measure inference time
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_iterations):
                _ = model(dummy_input)
                
                # Synchronize CUDA operations after each iteration
                if device.type == 'cuda':
                    torch.cuda.synchronize()
        
        # Calculate average time for this round
        elapsed_time = time.time() - start_time
        avg_time_ms = (elapsed_time / num_iterations) * 1000  # Convert to ms
        all_times.append(avg_time_ms)
    
    # Take median of all measurement rounds to reduce outlier impact
    import statistics
    median_time = statistics.median(all_times)
    
    # Log detailed measurement information
    logging.info(f"üìä Inference time measurements across {repeat_count} rounds (ms): {', '.join([f'{t:.2f}' for t in all_times])}")
    logging.info(f"üìä Median inference time: {median_time:.2f} ms (over {num_iterations} iterations per round)")
    
    return median_time

def optimize_model_for_inference(model, input_size=(1, 3, 32, 32), device=None):
    """
    Optimize a model for inference using various techniques.
    
    Args:
        model: PyTorch model to optimize
        input_size: Input tensor size for the model
        device: Device to run inference on
        
    Returns:
        Optimized model
    """
    if model is None:
        logging.error("‚ùå Model is None, cannot optimize")
        return model
        
    # Set model to evaluation mode
    model.eval()
    
    # Get device
    if device is None:
        try:
            device = next(model.parameters()).device
        except StopIteration:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Create dummy input
    dummy_input = torch.randn(input_size, device=device)
    
    # Âú®Ê≠§Â§ÑÈ¢ÑÂÖàËÆ°ÁÆóÂéüÂßãÊ®°ÂûãËæìÂá∫‰Ωú‰∏∫ÂèÇËÄÉÂü∫ÂáÜ
    try:
        with torch.no_grad():
            orig_output = model(dummy_input)
    except Exception as e:
        logging.error(f"‚ùå Unable to get original model output: {e}")
        return model
    
    # Try to use TorchScript optimization
    try:
        logging.info("üöÄ Optimizing model with TorchScript...")
        # Use torch.jit.trace for direct optimization
        with torch.no_grad():
            jit_model = torch.jit.trace(model, dummy_input)
            # Validate output
            jit_output = jit_model(dummy_input)
            if torch.allclose(orig_output, jit_output, rtol=1e-03, atol=1e-04):
                logging.info("‚úÖ TorchScript optimization successful")
                return jit_model
            else:
                logging.warning("‚ö†Ô∏è TorchScript model output differs significantly from original model, skipping optimization")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Failed to optimize model with TorchScript: {e}")
    
    # Try fusion of operations if TorchScript failed
    try:
        logging.info("üöÄ Trying operation fusion optimization...")
        # Create a deep copy to avoid modifying original
        import copy
        fused_model = copy.deepcopy(model)
        
        # Apply basic fusion optimizations manually (when applicable)
        for module in fused_model.modules():
            # Fuse BatchNorm into Conv layers where possible
            if hasattr(module, '_freeze_bn_stats'):
                module._freeze_bn_stats()
                
        # Test the fused model
        with torch.no_grad():
            fused_output = fused_model(dummy_input)
            if torch.allclose(orig_output, fused_output, rtol=1e-03, atol=1e-04):
                logging.info("‚úÖ Operation fusion successful")
                return fused_model
            else:
                logging.warning("‚ö†Ô∏è Fused model output differs significantly from original, skipping fusion")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Failed to apply operation fusion: {e}")
    
    # If all optimizations failed, return original model
    logging.info("‚ö†Ô∏è All optimization attempts failed, using original model")
    return model 