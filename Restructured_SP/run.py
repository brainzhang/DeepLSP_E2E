#!/usr/bin/env python3
# pyright: reportMissingImports=false, reportCallIssue=false, reportArgumentType=false

import os
import sys
import argparse
import logging
import torch
import json
from datetime import datetime
from pathlib import Path
import torch.onnx
import torch.nn as nn
import copy
import numpy as np
import time
from typing import Tuple, Dict, Any

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import modules from the project
from config.config import MODEL_MAPPING, get_input_size, get_experiment_dir
from utils.logging_setup import setup_logging, log_system_info
from data.datasets import get_dataloaders
from models.model_factory import create_model, fix_resnet_residual_blocks

# Import specific functions from modules
from core.training import train_model, fine_tune_model, evaluate_model
from core.pruning import StructuredPruning
from core.pruning_metrics import calculate_pruning_metrics
from utils.metrics import count_parameters, calculate_flops, measure_inference_time, optimize_model_for_inference
from utils.visualization import plot_training_history, save_results

# è¿™äº›å¯èƒ½éœ€è¦å•ç‹¬å®‰è£…
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    logging.warning("âš ï¸ ONNXRuntime not available. Install with: pip install onnxruntime-gpu")

try:
    import torch_pruning as tp
    from torch_pruning import MetaPruner
    TORCH_PRUNING_AVAILABLE = True
except ImportError:
    TORCH_PRUNING_AVAILABLE = False
    logging.warning("âš ï¸ torch_pruning not available. Install with: pip install torch-pruning")

# å°è¯•å¯¼å…¥TensorRT
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    logging.warning("âš ï¸ TensorRT not available. Follow https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html to install.")

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Run structured pruning experiments')
    
    # Basic experiment settings

    parser.add_argument('model', type=str, default='resnet18',
                        help='Model architecture to use (defaults to dataset default)')
    parser.add_argument('dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100', 'imagenet', 'fashionmnist'],
                        help='Dataset to use for experiments')
    parser.add_argument('--output-dir', type=str, default='/mnt/d/Ubuntu/phd/Prune_Methods/Restructured_SP/experiments',
                        help='Directory to save experiment results')
    
    # Training settings
    parser.add_argument('--base-train-epochs', type=int, default=100,
                        help='Number of epochs for base training')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                        help='Learning rate for training')
    parser.add_argument('--batch-size', type=int, default=128,
                        help='Batch size for training and evaluation')
    parser.add_argument('--weight-decay', type=float, default=5e-4,
                        help='Weight decay for training')
    parser.add_argument('--patience', type=int, default=20,
                        help='Early stopping patience')
    
    # Pruning settings
    parser.add_argument('--pruning-method', nargs='+', 
                        default=['channel', 'kernel', 'intra-kernel'],
                        choices=['channel', 'kernel', 'intra-kernel'],
                        help='Methods for pruning (can specify multiple methods)')
    parser.add_argument('--pruning-ratio', type=float, default=0.5,
                        help='Pruning ratio (fraction of channels to remove)')
    parser.add_argument('--global-pruning', action='store_true',
                        help='Use global pruning across all layers (default: layer-wise)')
    
    # Fine-tuning settings
    parser.add_argument('--finetune-epochs', type=int, default=10,
                        help='Number of epochs for fine-tuning after pruning')
    parser.add_argument('--finetune-lr', type=float, default=0.001,
                        help='Learning rate for fine-tuning')
    
    # Execution settings
    parser.add_argument('--no-cuda', action='store_true',
                        help='Disable CUDA even if available')
    parser.add_argument('--no-fp16', action='store_true',
                        help='Disable mixed precision training')
    parser.add_argument('--num-workers', type=int, default=4,
                        help='Number of worker threads for data loading')
    parser.add_argument('--resume', action='store_true',
                        help='Resume training from checkpoint if available')
    parser.add_argument('--skip-base-training', action='store_true',
                        help='Skip base training and start from a pretrained model')
    parser.add_argument('--skip-pruning', action='store_true',
                        help='Skip pruning step (for baseline evaluations)')
    parser.add_argument('--eval-only', action='store_true',
                        help='Only run evaluation on an existing model')
    
    # æ¨ç†ä¼˜åŒ–é€‰é¡¹
    parser.add_argument('--use-tensorrt', action='store_true',
                        help='Use TensorRT for optimized inference')
    parser.add_argument('--use-onnx', action='store_true',
                        help='Use ONNX Runtime for optimized inference')
    parser.add_argument('--align-channels', action='store_true',
                        help='Align pruned channels to hardware-friendly values (e.g., multiples of 32)')
    parser.add_argument('--hardware-align', type=int, default=32,
                        help='Channel alignment value for hardware-friendly pruning (default: 32)')
    parser.add_argument('--skip-rebuild', action='store_true',
                        help='Skip rebuilding the model structure after pruning (keep zero weights)')
    
    # Model loading/saving
    parser.add_argument('--save-model', action='store_true',
                        help='Save models during training')
    parser.add_argument('--model-path', type=str, default=None,
                        help='Path to a saved model to load')
    
    return parser.parse_args()

def run_experiment(args):
    """
    Run a complete pruning experiment based on provided arguments.
    
    Args:
        args: Command line arguments
    """
    # Set up timestamp for this experiment
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Select model architecture based on dataset if not specified
    if args.model is None:
        model_name = MODEL_MAPPING[args.dataset]['default']
    else:
        model_name = args.model
    
    # Create experiment directory and setup logging
    experiment_dir = get_experiment_dir(args.dataset, model_name, args.pruning_method, timestamp)
    log_path = setup_logging(args.dataset, model_name, timestamp)
    
    # Log experiment configuration
    logging.info(f"ğŸ§ª Starting experiment with {model_name} on {args.dataset}")
    logging.info(f"ğŸ§ª Pruning methods: {', '.join(args.pruning_method)}, ratio: {args.pruning_ratio}")
    
    # åˆå§‹åŒ–å…³é”®å˜é‡ï¼Œé¿å…æœªç»‘å®šé”™è¯¯
    pruned_model = None
    
    # Get device
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device, cuda_available = log_system_info()
    
    if not use_cuda:
        device = torch.device('cpu')
        logging.info("Using CPU for computation (CUDA disabled or not available)")
    
    # Set random seed for reproducibility
    torch.manual_seed(0)
    if cuda_available:
        torch.cuda.manual_seed_all(0)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # Load dataset
    logging.info(f"ğŸ“š Loading {args.dataset} dataset")
    data = get_dataloaders(
        dataset_name=args.dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    train_loader = data['train_loader']
    test_loader = data['test_loader']
    num_classes = data['num_classes']
    input_size = data['input_size']
    
    # Create results dictionary to store experiment metrics
    results = {
        'dataset': args.dataset,
        'model': model_name,
        'pruning_method': args.pruning_method,
        'pruning_ratio': args.pruning_ratio,
        'global_pruning': args.global_pruning,
        'timestamp': timestamp,
    }
    
    # Create and train the model (if not evaluating only)
    if not args.eval_only:
        if args.model_path is not None and os.path.exists(args.model_path):
            # Load existing model
            logging.info(f"â¬‡ï¸ Loading model from {args.model_path}")
            
            # Create model architecture
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=False,
                device=device
            )
            
            # Load saved weights
            model.load_state_dict(torch.load(args.model_path, map_location=device))
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            logging.info(f"âœ… Model loaded successfully: {model_name}")
        elif args.skip_base_training:
            # Create model with pretrained weights
            logging.info(f"â¬‡ï¸ Creating model with pretrained weights: {model_name}")
            
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=True,
                device=device
            )
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            # Perform a quick evaluation before pruning
            orig_loss, orig_acc = evaluate_model(model, test_loader)
            logging.info(f"ğŸ“Š Pretrained model - Loss: {orig_loss:.4f}, Accuracy: {orig_acc:.2f}%")
            results['pretrained_accuracy'] = orig_acc
        else:
            # Create and train model from scratch
            logging.info(f"ğŸ—ï¸ Creating and training model from scratch: {model_name}")
            
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=False,
                device=device
            )
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            # Train the model
            use_fp16 = not args.no_fp16
            train_results = train_model(
                model=model,
                train_loader=train_loader,
                val_loader=test_loader,
                num_epochs=args.base_train_epochs,
                learning_rate=args.learning_rate,
                weight_decay=args.weight_decay,
                patience=args.patience,
                fp16=use_fp16
            )
            
            # Save training history
            results['base_training'] = {
                'epochs': len(train_results['history']['loss']),
                'best_epoch': train_results['best_epoch'],
                'best_val_acc': train_results['best_val_acc'],
                'training_time': train_results['training_time']
            }
            
            # Plot training history
            plot_path = os.path.join(experiment_dir, f"{model_name}_training_history.png")
            plot_training_history(train_results['history'], plot_path)
            
            # Save the trained model if requested
            if args.save_model:
                model_save_path = os.path.join(experiment_dir, f"{model_name}_base_trained.pth")
                torch.save(model.state_dict(), model_save_path)
                logging.info(f"ğŸ’¾ Base trained model saved to: {model_save_path}")
        
        # Calculate model size and FLOPs before pruning
        orig_params = count_parameters(model)
        orig_flops = calculate_flops(model, input_size)
        
        # åˆ›å»ºæ¨¡å‹å‰¯æœ¬ç”¨äºæ¨ç†æµ‹é‡ï¼Œä¿ç•™åŸå§‹æ¨¡å‹
        logging.info("ğŸ“Š å‡†å¤‡æµ‹é‡åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´...")
        orig_model_copy = copy.deepcopy(model)
        
        # ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦
        logging.info("ğŸš€ ä¼˜åŒ–åŸå§‹æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦...")
        optimized_original = optimize_model_for_inference(orig_model_copy, input_size=input_size)
        
        # æµ‹é‡æ¨ç†æ—¶é—´
        orig_inference_time = measure_inference_time(optimized_original, input_size=input_size, use_jit=True)
        
        results['original_model'] = {
            'parameters': orig_params,
            'flops': orig_flops,
            'inference_time_ms': orig_inference_time
        }
        
        logging.info(f"ğŸ“Š Original model - Parameters: {orig_params:,}, FLOPs: {orig_flops/1e6:.2f}M, Inference time: {orig_inference_time:.2f}ms")
        
        # Skip pruning if requested
        if not args.skip_pruning:
            # Apply pruning methods
            # ä½¿ç”¨æ ‡å‡†å‰ªææ–¹æ³•
            logging.info(f"âœ‚ï¸ Applying pruning methods: {', '.join(args.pruning_method)}, ratio: {args.pruning_ratio}")
            
            pruner = StructuredPruning(model, pruning_ratio=args.pruning_ratio)
            pruned_model = model
            
            # åº”ç”¨å‰ªæï¼ˆæ­¤æ—¶åªæ˜¯ç½®é›¶æƒé‡ï¼‰
            for method in args.pruning_method:
                logging.info(f"âœ‚ï¸ Applying {method} pruning method")
                pruned_model = pruner.prune_model(method=method, global_pruning=args.global_pruning)
            
            # å¦‚æœtorch_pruningå¯ç”¨ï¼Œä½¿ç”¨ä¸“ä¸šå·¥å…·è¿›è¡Œç»“æ„åŒ–å‰ªæ
            if not args.skip_rebuild and TORCH_PRUNING_AVAILABLE:
                logging.info("ğŸ”„ ä½¿ç”¨torch_pruningä¸“ä¸šå·¥å…·è¿›è¡Œç»“æ„åŒ–å‰ªæ...")
                try:
                    # ä½¿ç”¨ä¸“ä¸šå‰ªæå·¥å…· (å…¨å±€å‡½æ•°)
                    pruned_model = prune_model_properly(pruned_model, args.pruning_ratio, args.dataset)
                except Exception as e:
                    logging.error(f"âŒ ä¸“ä¸šå‰ªæå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
                    logging.warning("âš ï¸ å›é€€åˆ°åŸºæœ¬å‰ªææ–¹æ³•")
            
            # è¯„ä¼°ç½®é›¶æƒé‡åçš„æ¨¡å‹
            masked_loss, masked_acc = evaluate_model(pruned_model, test_loader)
            logging.info(f"ğŸ“Š Pruned model (with masks) - Loss: {masked_loss:.4f}, Accuracy: {masked_acc:.2f}%")
            
            results['masked_model'] = {
                'accuracy': masked_acc,
                'loss': masked_loss
            }
            
            # æ›´æ–°æ¨¡å‹å¼•ç”¨
            model = pruned_model
            
            # Fine-tune the pruned model if requested
            if args.finetune_epochs > 0:
                logging.info(f"ğŸ”„ Fine-tuning pruned model for {args.finetune_epochs} epochs")
                
                # Fine-tune with mixed precision if enabled
                use_fp16 = not args.no_fp16
                finetune_results = fine_tune_model(
                    model=model,
                    train_loader=train_loader,
                    val_loader=test_loader,
                    num_epochs=args.finetune_epochs,
                    learning_rate=args.finetune_lr,
                    fp16=use_fp16
                )
                
                # Save fine-tuning history
                results['fine_tuning'] = {
                    'epochs': len(finetune_results['history']['loss']),
                    'best_epoch': finetune_results['best_epoch'],
                    'best_val_acc': finetune_results['best_val_acc'],
                    'training_time': finetune_results['training_time']
                }
                
                # Plot fine-tuning history
                plot_path = os.path.join(experiment_dir, f"{model_name}_finetuning_history.png")
                plot_training_history(finetune_results['history'], plot_path)
                
                # Save the fine-tuned model if requested
                if args.save_model:
                    model_save_path = os.path.join(experiment_dir, f"{model_name}_finetuned.pth")
                    torch.save(model.state_dict(), model_save_path)
                    logging.info(f"ğŸ’¾ Fine-tuned model saved to: {model_save_path}")
            
            # Rebuild the model to remove pruned channels
            logging.info("ğŸ”¨ Calculating pruning metrics")
            
            # Get metrics without rebuilding the model
            metrics = calculate_pruning_metrics(
                model=model,
                input_size=input_size
            )
            
            # Keep the same model instead of rebuilding
            rebuilt_model = model
            
            # Update model to rebuilt version
            model = rebuilt_model
            
            # Validate the rebuilt model with a single forward pass
            try:
                model.eval()
                with torch.no_grad():
                    dummy_input = torch.randn(1, input_size[1], input_size[2], input_size[3], device=device)
                    _ = model(dummy_input)
                logging.info("âœ… Validated rebuilt model with test forward pass")
                
                # Measure inference time after pruning
                logging.info("ğŸ“Š Measuring inference time after pruning...")
                
                # ä½¿ç”¨å½“å‰æ¨¡å‹ä½œä¸ºè¯„ä¼°åŸºå‡†
                model_for_eval = model  # ä¿å­˜ç”¨äºè¯„ä¼°çš„æ ‡å‡†æ¨¡å‹
                
                # å¦‚æœå¯ç”¨äº†é€šé“å¯¹é½
                if args.align_channels:
                    logging.info(f"ğŸ”§ å¯¹é½æ¨¡å‹é€šé“åˆ°{args.hardware_align}çš„å€æ•°...")
                    model = align_pruning_channels(model, align_to=args.hardware_align)
                
                # ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦
                logging.info("ğŸš€ ä¼˜åŒ–æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦...")
                
                # åˆ›å»ºONNXæ¨¡å‹è·¯å¾„
                onnx_path = os.path.join(experiment_dir, f"{model_name}_pruned.onnx")
                
                # ç¡®ä¿å®éªŒç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(onnx_path), exist_ok=True)
                
                # æµ‹é‡æ¨ç†æ—¶é—´
                pruned_inference_time = 0.0
                
                # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–æ–¹æ³•æµ‹é‡æ¨ç†æ—¶é—´
                if args.use_tensorrt and TENSORRT_AVAILABLE:
                    logging.info("ğŸš€ ä½¿ç”¨TensorRTè¿›è¡Œæ¨ç†ä¼˜åŒ–...")
                    
                    # åˆ›å»ºTensorRTå¼•æ“
                    engine_path = os.path.join(experiment_dir, f"{model_name}_pruned.engine")
                    engine = create_tensorrt_engine(
                        model, 
                        input_size=input_size, 
                        onnx_path=onnx_path, 
                        engine_path=engine_path
                    )
                    
                    # ä½¿ç”¨TensorRTæµ‹é‡æ¨ç†æ—¶é—´
                    if engine is not None:
                        pruned_inference_time = measure_tensorrt_inference_time(
                            engine,
                            input_size=input_size
                        )
                    else:
                        logging.warning("âš ï¸ TensorRTå¼•æ“åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°PyTorchæ¨ç†")
                        optimized_model = optimize_model_for_inference(copy.deepcopy(model), input_size=input_size)
                        pruned_inference_time = measure_inference_time(optimized_model, input_size=input_size, use_jit=True)
                
                elif args.use_onnx and ONNX_AVAILABLE:
                    logging.info("ğŸš€ ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ä¼˜åŒ–...")
                    
                    # å¯¼å‡ºä¸ºONNXæ ¼å¼
                    dummy_input = torch.randn(input_size, device=device)
                    torch.onnx.export(
                        model,
                        dummy_input,
                        onnx_path,
                        export_params=True,
                        opset_version=13,
                        do_constant_folding=True,
                        input_names=['input'],
                        output_names=['output'],
                        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
                    )
                    
                    # ä½¿ç”¨ONNX Runtimeæµ‹é‡æ¨ç†æ—¶é—´
                    pruned_inference_time = measure_onnx_inference_time(
                        onnx_path,
                        input_size=input_size,
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                    
                else:
                    # ä½¿ç”¨PyTorch JITä¼˜åŒ–
                    optimized_model = optimize_model_for_inference(copy.deepcopy(model), input_size=input_size)
                    pruned_inference_time = measure_inference_time(optimized_model, input_size=input_size, use_jit=True)
                
                metrics['inference_time_ms'] = pruned_inference_time
                
                # Update inference time speedup
                if 'original_params' in metrics and metrics.get('inference_time_ms', 0) > 0:
                    orig_time = results['original_model']['inference_time_ms']
                    time_speedup = 100.0 * (1.0 - pruned_inference_time / orig_time) if orig_time > 0 else 0
                    metrics['inference_speedup'] = time_speedup
                    
                    # Add inference time reduction metric (negative when time increases)
                    time_reduction = 100.0 * (orig_time - pruned_inference_time) / orig_time if orig_time > 0 else 0
                    metrics['inference_time_reduction'] = time_reduction
                    
                    logging.info(f"ğŸ“Š æ¨ç†æ—¶é—´å˜åŒ–: {orig_time:.2f}ms â†’ {pruned_inference_time:.2f}ms")
                    logging.info(f"ğŸ“Š æ¨ç†æ—¶é—´åŠ é€Ÿç‡: {time_speedup:.2f}%, æ—¶é—´å‡å°‘ç‡: {time_reduction:.2f}%")
                
            except Exception as e:
                logging.error(f"âŒ Error validating rebuilt model: {e}")
                # Revert to original model
                logging.warning("âš ï¸ Reverting to original model due to rebuilding error")
                model = pruned_model
            
            # Save metrics from pruning
            results['pruning'] = metrics
            
            # å¦‚æœå®é™…æ‰§è¡Œäº†ç»“æ„é‡å»ºï¼Œé‡æ–°è®¡ç®—FLOPs
            if not args.skip_rebuild:
                # é‡æ–°è®¡ç®—å‚æ•°å’ŒFLOPs
                new_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                param_reduction = 100.0 * (1.0 - new_params / orig_params) if orig_params > 0 else 0
                
                # é‡æ–°è®¡ç®—FLOPs
                new_flops = calculate_flops(model, input_size)
                flops_reduction = 100.0 * (1.0 - new_flops / orig_flops) if orig_flops > 0 else 0
                
                # æ›´æ–°æŒ‡æ ‡
                metrics['pruned_params'] = new_params
                metrics['param_reduction'] = param_reduction
                metrics['pruned_flops'] = new_flops
                metrics['flops_reduction'] = flops_reduction
                
                logging.info(f"ğŸ“Š é‡å»ºå - å‚æ•°: {new_params:,} ({param_reduction:.2f}% å‡å°‘)")
                logging.info(f"ğŸ“Š é‡å»ºå - FLOPs: {new_flops/1e6:.2f}M ({flops_reduction:.2f}% å‡å°‘)")
            
            # Evaluate final model - ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè¯„ä¼°
            final_loss, final_acc = evaluate_model(model, test_loader)
            logging.info(f"ğŸ“Š Final rebuilt model - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%")
            
            # Use metrics from the pruned model for final model parameters
            results['final_model'] = {
                'accuracy': final_acc,
                'loss': final_loss,
                'parameters': metrics.get('pruned_params', orig_params),
                'flops': metrics.get('pruned_flops', orig_flops),
                'inference_time_ms': metrics.get('inference_time_ms', 0),
                'param_reduction': metrics.get('param_reduction', 0.0),
                'flops_reduction': metrics.get('flops_reduction', 0.0),
                'inference_speedup': metrics.get('inference_speedup', 0.0),
                'inference_time_reduction': metrics.get('inference_time_reduction', 0.0)
            }
            
    else:
        # Evaluation only mode
        if args.model_path is None:
            logging.error("âŒ Model path must be provided for evaluation-only mode")
            return None, None
        
        if not os.path.exists(args.model_path):
            logging.error(f"âŒ Model file not found: {args.model_path}")
            return None, None
        
        # Create model architecture
        model = create_model(
            model_name=model_name,
            dataset=args.dataset,
            num_classes=num_classes,
            pretrained=False,
            device=device
        )
        
        # Load saved weights
        model.load_state_dict(torch.load(args.model_path, map_location=device))
        
        # Fix any potential issues in residual blocks
        fix_resnet_residual_blocks(model, device)
        
        logging.info(f"âœ… Model loaded successfully: {model_name}")
    
    # Final evaluation
    logging.info("ğŸ“Š æ‰§è¡Œæœ€ç»ˆè¯„ä¼°...")
    final_loss, final_acc = evaluate_model(model, test_loader)
    logging.info(f"ğŸ“Š Final evaluation - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%")
    
    # Do NOT recalculate parameters and FLOPs here - use values from pruning metrics
    if 'final_model' not in results:
        results['final_model'] = {}
    
    # Update only accuracy and loss, keeping pruning metrics intact
    results['final_model'].update({
        'accuracy': final_acc,
        'loss': final_loss
    })
    
    # If we're in evaluation-only mode or skipped pruning, we need to calculate metrics
    if args.eval_only or args.skip_pruning:
        final_params = count_parameters(model)
        final_flops = calculate_flops(model, input_size)
        
        results['final_model'].update({
            'parameters': final_params,
            'flops': final_flops
        })
        
        # Calculate reduction ratios if original model is available
        if 'original_model' in results:
            orig_params = results['original_model']['parameters']
            orig_flops = results['original_model']['flops']
            
            if orig_params > 0:
                param_reduction = 100.0 * (1.0 - final_params / orig_params)
                results['final_model']['param_reduction'] = param_reduction
                
                if orig_flops > 0:
                    flops_reduction = 100.0 * (1.0 - final_flops / orig_flops)
                    results['final_model']['flops_reduction'] = flops_reduction
                    
                    logging.info(f"ğŸ“Š Model size reduction: {param_reduction:.2f}%, FLOPs reduction: {flops_reduction:.2f}%")
    # For pruned models, use the pruning metrics directly (already stored in results['final_model'] earlier)
    else:
        logging.info(f"ğŸ“Š Model size reduction: {results['final_model']['param_reduction']:.2f}%, " 
                    f"FLOPs reduction: {results['final_model']['flops_reduction']:.2f}%")
    
    # Save results
    results_path = os.path.join(experiment_dir, f"{model_name}_results.json")
    save_results(results, results_path)
    logging.info(f"ğŸ’¾ Experiment results saved to: {results_path}")
    
    # Create a summary
    logging.info(f"\n{'='*60}\nğŸ“ EXPERIMENT SUMMARY\n{'='*60}")
    logging.info(f"Model: {model_name}, Dataset: {args.dataset}")
    logging.info(f"Pruning methods: {', '.join(args.pruning_method)}, Ratio: {args.pruning_ratio}")
    
    if 'original_model' in results:
        logging.info(f"Original model - Parameters: {results['original_model']['parameters']:,}, "
                    f"FLOPs: {results['original_model']['flops']/1e6:.2f}M, "
                    f"Inference time: {results['original_model'].get('inference_time_ms', 0):.2f}ms")
    
    if 'final_model' in results:
        logging.info(f"Final model - Parameters: {results['final_model']['parameters']:,}, "
                    f"FLOPs: {results['final_model']['flops']/1e6:.2f}M, "
                    f"Inference time: {results['final_model'].get('inference_time_ms', 0):.2f}ms")
        
        if 'param_reduction' in results['final_model']:
            logging.info(f"Parameter reduction: {results['final_model']['param_reduction']:.2f}%")
        
        if 'flops_reduction' in results['final_model']:
            logging.info(f"FLOPs reduction: {results['final_model']['flops_reduction']:.2f}%")
        
        if 'inference_speedup' in results['final_model']:
            logging.info(f"Inference speedup: {results['final_model']['inference_speedup']:.2f}%")
        
        if 'inference_time_reduction' in results['final_model']:
            logging.info(f"Inference time reduction: {results['final_model']['inference_time_reduction']:.2f}%")
        
        logging.info(f"Final accuracy: {results['final_model']['accuracy']:.2f}%")
    
    logging.info(f"{'='*60}")
    
    return results, model

def create_tensorrt_engine(model, input_size, onnx_path=None, engine_path=None, precision='fp16'):
    """
    å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“ä»¥åŠ é€Ÿæ¨ç†
    
    Args:
        model: PyTorchæ¨¡å‹
        input_size: è¾“å…¥å°ºå¯¸ (batch_size, channels, height, width)
        onnx_path: ONNXæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        engine_path: TensorRTå¼•æ“ä¿å­˜è·¯å¾„
        precision: ç²¾åº¦æ¨¡å¼ ('fp32', 'fp16', 'int8')
        
    Returns:
        TensorRTå¼•æ“
    """
    if not TENSORRT_AVAILABLE:
        logging.error("âŒ TensorRT not available for optimized inference")
        return None
    
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å¦‚æœæœªæä¾›ONNXè·¯å¾„ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
    if onnx_path is None:
        import tempfile
        temp_dir = tempfile.gettempdir()
        onnx_path = os.path.join(temp_dir, f"temp_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.onnx")
    
    # å¦‚æœæœªæä¾›å¼•æ“è·¯å¾„ï¼Œæ ¹æ®ONNXè·¯å¾„åˆ›å»º
    if engine_path is None:
        engine_path = onnx_path.replace(".onnx", ".engine")
    
    # åˆ›å»ºéšæœºè¾“å…¥
    device = next(model.parameters()).device
    dummy_input = torch.randn(input_size, device=device)
    
    # å…ˆå¯¼å‡ºä¸ºONNXæ ¼å¼
    logging.info(f"ğŸ“¦ å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼: {onnx_path}")
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=13,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )
    
    # åˆ›å»ºTensorRTå¼•æ“
    logging.info(f"ğŸš€ æ„å»ºTensorRTå¼•æ“: {engine_path}")
    
    try:
        # ç¡®ä¿TensorRTå·²æˆåŠŸå¯¼å…¥
        import tensorrt as trt  # pyright: ignore[reportAttributeAccessIssue]
            
        # åˆ›å»ºTensorRT builderå’Œç½‘ç»œ
        logger = trt.Logger(trt.Logger.WARNING)  # pyright: ignore[reportAttributeAccessIssue]
        builder = trt.Builder(logger)  # pyright: ignore[reportAttributeAccessIssue]
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))  # pyright: ignore[reportAttributeAccessIssue]
        parser = trt.OnnxParser(network, logger)  # pyright: ignore[reportAttributeAccessIssue]
        
        # è§£æONNXæ–‡ä»¶
        with open(onnx_path, 'rb') as f:
            if not parser.parse(f.read()):
                logging.error("âŒ è§£æONNXæ–‡ä»¶å¤±è´¥")
                for error in range(parser.num_errors):
                    logging.error(f"Error {error}: {parser.get_error(error)}")
                return None
        
        # é…ç½®TensorRT
        config = builder.create_builder_config()
        # æ–°ç‰ˆTensorRT APIä½¿ç”¨set_memory_pool_limit
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # pyright: ignore[reportAttributeAccessIssue]
        
        # ä¸ºåŠ¨æ€æ‰¹é‡å¤§å°åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶
        profile = builder.create_optimization_profile()
        # è®¾ç½®æ‰¹é‡å¤§å°èŒƒå›´: æœ€å°ã€æœ€ä¼˜ã€æœ€å¤§
        min_batch = 1
        opt_batch = input_size[0]  # ä¼˜åŒ–å¤§å°ä¸ºè¾“å…¥æ‰¹é‡å¤§å°
        max_batch = input_size[0] * 2  # æœ€å¤§æ‰¹é‡å¤§å°è®¾ä¸ºè¾“å…¥çš„2å€
        
        # é…ç½®'input'å¼ é‡çš„ä¼˜åŒ–å‚æ•°
        profile.set_shape(
            "input",  # è¾“å…¥åç§°
            (min_batch, input_size[1], input_size[2], input_size[3]),  # æœ€å°å°ºå¯¸
            (opt_batch, input_size[1], input_size[2], input_size[3]),  # æœ€ä¼˜å°ºå¯¸
            (max_batch, input_size[1], input_size[2], input_size[3])   # æœ€å¤§å°ºå¯¸
        )
        config.add_optimization_profile(profile)
        
        # è®¾ç½®ç²¾åº¦
        if precision == 'fp16' and builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)  # pyright: ignore[reportAttributeAccessIssue]
            logging.info("âš™ï¸ å¯ç”¨FP16ç²¾åº¦")
        elif precision == 'int8' and builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)  # pyright: ignore[reportAttributeAccessIssue]
            logging.info("âš™ï¸ å¯ç”¨INT8ç²¾åº¦")
        
        # æ„å»ºå¼•æ“
        serialized_engine = builder.build_serialized_network(network, config)
        with open(engine_path, "wb") as f:
            f.write(serialized_engine)
        
        # åˆ›å»ºè¿è¡Œæ—¶å’Œå¼•æ“
        runtime = trt.Runtime(logger)  # pyright: ignore[reportAttributeAccessIssue]
        with open(engine_path, "rb") as f:
            engine = runtime.deserialize_cuda_engine(f.read())
            
        logging.info("âœ… TensorRTå¼•æ“åˆ›å»ºæˆåŠŸ")
        return engine
    
    except ImportError as e:
        logging.error(f"âŒ TensorRTå¯¼å…¥å¤±è´¥: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ åˆ›å»ºTensorRTå¼•æ“å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return None

def measure_tensorrt_inference_time(engine, input_size, num_iterations=100):
    """
    ä½¿ç”¨TensorRTå¼•æ“æµ‹é‡æ¨ç†æ—¶é—´
    
    Args:
        engine: TensorRTå¼•æ“
        input_size: è¾“å…¥å°ºå¯¸ (batch_size, channels, height, width)
        num_iterations: æµ‹é‡è¿­ä»£æ¬¡æ•°
        
    Returns:
        float: å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    """
    if engine is None:
        logging.error("âŒ æ— æ•ˆçš„TensorRTå¼•æ“")
        return 0
    
    # æ£€æŸ¥å¿…è¦çš„åº“æ˜¯å¦å·²å¯¼å…¥
    if not TENSORRT_AVAILABLE:
        logging.error("âŒ TensorRTæœªæ­£ç¡®å¯¼å…¥")
        return 0
        
    try:
        # é¿å…é™æ€åˆ†æå™¨çš„è­¦å‘Šï¼ŒåŠ¨æ€å¯¼å…¥æ‰€éœ€æ¨¡å—
        import importlib
        cuda_module = importlib.import_module("pycuda.driver")
        
        # åˆ›å»ºCUDAä¸Šä¸‹æ–‡
        context = engine.create_execution_context()
        
        # å‡†å¤‡è¾“å…¥å’Œè¾“å‡ºå†…å­˜ - ç›´æ¥ä½¿ç”¨numpyåˆ›å»ºæ•°ç»„ï¼Œé¿å…ç±»å‹é—®é¢˜
        h_input = np.zeros(input_size, dtype=np.float32)
        for i in range(input_size[0]):
            for c in range(input_size[1]):
                for h in range(input_size[2]):
                    for w in range(input_size[3]):
                        h_input[i,c,h,w] = np.random.random()
        
        # è·å–è¾“å‡ºå½¢çŠ¶ - ä½¿ç”¨æ–°ç‰ˆTensorRT API
        # æ—§ç‰ˆ: output_shape = engine.get_binding_shape(1)
        # ä½¿ç”¨contextæˆ–engineè·å–ç»‘å®šç»´åº¦çš„æ›¿ä»£æ–¹æ³•
        binding_idx = 1  # è¾“å‡ºç»‘å®šç´¢å¼•
        if hasattr(engine, 'get_binding_dimensions'):
            # å°è¯•ä½¿ç”¨get_binding_dimensions
            output_dims = engine.get_binding_dimensions(binding_idx)
            output_shape = (input_size[0], output_dims[1])  # å‡è®¾ä¸º(N, C)æ ¼å¼
        elif hasattr(context, 'get_binding_shape'):
            # å°è¯•ä»ä¸Šä¸‹æ–‡è·å–
            output_shape = context.get_binding_shape(binding_idx)
        else:
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ - å‡è®¾è¾“å‡ºä¸æ¨¡å‹å¯¹åº”
            logging.warning("âš ï¸ æ— æ³•è·å–è¾“å‡ºå½¢çŠ¶ï¼Œä½¿ç”¨é»˜è®¤å€¼(N, 1000)")
            output_shape = (input_size[0], 1000)  # å‡è®¾ä¸º1000ç±»åˆ†ç±»
        
        logging.info(f"ğŸ“Š TensorRTè¾“å‡ºå½¢çŠ¶: {output_shape}")
        h_output = np.zeros((input_size[0], output_shape[1]), dtype=np.float32)
        
        # ä½¿ç”¨getattråŠ¨æ€è·å–PyCUDAå‡½æ•°ï¼Œé¿å…é™æ€åˆ†æå™¨è­¦å‘Š
        try:
            # åˆ†é…GPUå†…å­˜
            mem_alloc_fn = getattr(cuda_module, "mem_alloc")
            d_input = mem_alloc_fn(h_input.nbytes)
            d_output = mem_alloc_fn(h_output.nbytes)
            
            # åˆ›å»ºCUDAæµ
            stream_cls = getattr(cuda_module, "Stream")
            stream = stream_cls()
            
            # å†…å­˜æ‹·è´å‡½æ•°
            htod_async_fn = getattr(cuda_module, "memcpy_htod_async")
            dtoh_async_fn = getattr(cuda_module, "memcpy_dtoh_async")
            
            # é¢„çƒ­
            for _ in range(10):
                htod_async_fn(d_input, h_input, stream)
                context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
                dtoh_async_fn(h_output, d_output, stream)
                stream.synchronize()
            
            # æµ‹é‡æ—¶é—´
            start_time = time.time()
            for _ in range(num_iterations):
                htod_async_fn(d_input, h_input, stream)
                context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
                dtoh_async_fn(h_output, d_output, stream)
                stream.synchronize()
        except (AttributeError, ImportError) as e:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šæ–¹æ³•æˆ–å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
            logging.warning(f"âš ï¸ ä½¿ç”¨åŒæ­¥æ¨ç†æ¥å£: {e}")
            
            # ç®€åŒ–çš„åŒæ­¥æ¨ç†ç‰ˆæœ¬
            import pycuda.driver
            
            # ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„æ¥å£ï¼Œæ·»åŠ type:ignoreæ³¨é‡Šå¿½ç•¥é™æ€åˆ†æé”™è¯¯
            d_input = pycuda.driver.mem_alloc(h_input.nbytes)  # type: ignore
            d_output = pycuda.driver.mem_alloc(h_output.nbytes)  # type: ignore
            
            # é¢„çƒ­
            for _ in range(10):
                pycuda.driver.memcpy_htod(d_input, h_input)  # type: ignore
                context.execute_v2(bindings=[int(d_input), int(d_output)])
                pycuda.driver.memcpy_dtoh(h_output, d_output)  # type: ignore
            
            # æµ‹é‡æ—¶é—´
            start_time = time.time()
            for _ in range(num_iterations):
                pycuda.driver.memcpy_htod(d_input, h_input)  # type: ignore
                context.execute_v2(bindings=[int(d_input), int(d_output)])
                pycuda.driver.memcpy_dtoh(h_output, d_output)  # type: ignore
        
        elapsed_time = time.time() - start_time
        avg_time_ms = (elapsed_time / num_iterations) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        logging.info(f"ğŸ“Š TensorRTå¹³å‡æ¨ç†æ—¶é—´: {avg_time_ms:.2f} ms ({num_iterations} æ¬¡è¿­ä»£)")
        
        # é‡Šæ”¾èµ„æº
        del context
        
        return avg_time_ms
    except ImportError as e:
        logging.error(f"âŒ PyCUDAå¯¼å…¥å¤±è´¥: {e}")
        return 0
    except Exception as e:
        logging.error(f"âŒ TensorRTæ¨ç†æµ‹é‡å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return 0

def measure_onnx_inference_time(onnx_path, input_size, num_iterations=100, device='cuda'):
    """
    ä½¿ç”¨ONNX Runtimeæµ‹é‡æ¨ç†æ—¶é—´
    
    Args:
        onnx_path: ONNXæ¨¡å‹è·¯å¾„
        input_size: è¾“å…¥å°ºå¯¸ (batch_size, channels, height, width)
        num_iterations: æµ‹é‡è¿­ä»£æ¬¡æ•°
        device: è¿è¡Œè®¾å¤‡ ('cuda' æˆ– 'cpu')
        
    Returns:
        float: å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    """
    if not ONNX_AVAILABLE:
        logging.error("âŒ ONNX Runtime not available for optimized inference")
        return 0
    
    if not os.path.exists(onnx_path):
        logging.error(f"âŒ ONNXæ¨¡å‹ä¸å­˜åœ¨: {onnx_path}")
        return 0
    
    # åˆ›å»ºONNXä¼šè¯
    logging.info(f"ğŸ“¦ åŠ è½½ONNXæ¨¡å‹: {onnx_path}")
    try:
        import onnxruntime as ort
            
        if device == 'cuda':
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        else:
            providers = ['CPUExecutionProvider']
            
        session = ort.InferenceSession(onnx_path, providers=providers)
        
        # å‡†å¤‡è¾“å…¥ - ç›´æ¥åˆ›å»ºnumpyæ•°ç»„
        input_name = session.get_inputs()[0].name
        dummy_input = np.zeros(input_size, dtype=np.float32)
        for i in range(input_size[0]):
            for c in range(input_size[1]):
                for h in range(input_size[2]):
                    for w in range(input_size[3]):
                        dummy_input[i,c,h,w] = np.random.random()
        
        # é¢„çƒ­
        for _ in range(10):
            _ = session.run(None, {input_name: dummy_input})
        
        # æµ‹é‡æ—¶é—´
        start_time = time.time()
        for _ in range(num_iterations):
            _ = session.run(None, {input_name: dummy_input})
        
        elapsed_time = time.time() - start_time
        avg_time_ms = (elapsed_time / num_iterations) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        logging.info(f"ğŸ“Š ONNX Runtimeå¹³å‡æ¨ç†æ—¶é—´: {avg_time_ms:.2f} ms ({num_iterations} æ¬¡è¿­ä»£)")
        
        return avg_time_ms
    
    except ImportError as e:
        logging.error(f"âŒ ONNX Runtimeå¯¼å…¥å¤±è´¥: {e}")
        return 0
    except Exception as e:
        logging.error(f"âŒ ONNX Runtimeæ¨ç†å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return 0

# æ·»åŠ æ–°çš„é€šé“å¯¹é½å‰ªæåŠŸèƒ½
def round_to_multiple(number, multiple):
    """å°†æ•°å­—å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„å€æ•°"""
    return multiple * round(number / multiple)

def align_pruning_channels(model, align_to=32):
    """
    è°ƒæ•´æ¨¡å‹å„å±‚çš„è¾“å‡ºé€šé“æ•°ï¼Œä½¿å…¶å¯¹é½åˆ°æŒ‡å®šçš„å€æ•°
    è¿™å¯ä»¥å¤§å¹…æé«˜GPUæ¨ç†æ€§èƒ½
    
    Args:
        model: PyTorchæ¨¡å‹
        align_to: å¯¹é½çš„é€šé“æ•°å€æ•°
        
    Returns:
        è°ƒæ•´åçš„æ¨¡å‹
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # è·å–å½“å‰é€šé“æ•°
            out_channels = module.out_channels
            # å‘ä¸‹å¯¹é½åˆ°æœ€è¿‘çš„å€æ•°
            aligned_channels = (out_channels // align_to) * align_to
            # å¦‚æœå¯¹é½ä¼šå‡å°‘å¤ªå¤šé€šé“ï¼Œåˆ™å‘ä¸Šå¯¹é½
            if out_channels - aligned_channels > align_to / 2:
                aligned_channels += align_to
            
            # å¦‚æœéœ€è¦è°ƒæ•´ä¸”ä¸ä¼šä½¿é€šé“æ•°å˜ä¸º0
            if aligned_channels != out_channels and aligned_channels > 0:
                logging.info(f"âš™ï¸ å¯¹é½å±‚ {name} çš„è¾“å‡ºé€šé“: {out_channels} -> {aligned_channels}")
                
                # åˆ›å»ºæ–°çš„å·ç§¯å±‚
                aligned_conv = nn.Conv2d(
                    module.in_channels,
                    aligned_channels,
                    module.kernel_size,
                    stride=module.stride,
                    padding=module.padding,
                    dilation=module.dilation,
                    groups=module.groups,
                    bias=module.bias is not None,
                    padding_mode=module.padding_mode
                )
                
                # å¤åˆ¶åŸå§‹æƒé‡
                with torch.no_grad():
                    if aligned_channels > out_channels:
                        # æ‰©å±•é€šé“
                        aligned_conv.weight[:out_channels] = module.weight
                        # å¤„ç†biasï¼Œé¿å…å¯¹Noneä½¿ç”¨ä¸‹æ ‡
                        if module.bias is not None and aligned_conv.bias is not None:
                            aligned_conv.bias[:out_channels] = module.bias
                    else:
                        # æ”¶ç¼©é€šé“
                        aligned_conv.weight = nn.Parameter(module.weight[:aligned_channels])
                        # å¤„ç†biasï¼Œé¿å…å¯¹Noneä½¿ç”¨ä¸‹æ ‡
                        if module.bias is not None and aligned_conv.bias is not None:
                            aligned_conv.bias = nn.Parameter(module.bias[:aligned_channels])
                
                # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                # æ³¨æ„ï¼šè¿™ç§ç›´æ¥æ›¿æ¢å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç½‘ç»œä¸­
                # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ä½¿ç”¨æ›´å®Œå–„çš„å±‚æ›¿æ¢æ–¹æ³•
                setattr(model, name.split('.')[-1], aligned_conv)
    
    return model

def rebuild_pruned_model(model):
    """
    åˆ›å»ºç»“æ„åŒ–çš„å‰ªææ¨¡å‹ï¼ŒçœŸæ­£ç§»é™¤é›¶æƒé‡é€šé“ï¼Œè€Œä¸ä»…ä»…æ˜¯å°†å®ƒä»¬ç½®ä¸ºé›¶ã€‚
    å®ç°æ–¹å¼æ˜¯æ‰¾å‡ºéé›¶é€šé“ï¼Œåˆ›å»ºæ–°çš„æ›´å°çš„ç½‘ç»œã€‚
    åŒæ—¶å¤„ç†åç»­çš„BatchNormå±‚ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…ã€‚
    
    Args:
        model: å·²ç»åº”ç”¨äº†æƒé‡æ©ç çš„æ¨¡å‹
        
    Returns:
        new_model: ç»“æ„æ›´å°çš„æ–°æ¨¡å‹
    """
    logging.info("âš™ï¸ æ­£åœ¨é‡å»ºæ¨¡å‹ï¼Œç§»é™¤é›¶æƒé‡é€šé“...")
    
    # è·å–åŸå§‹æ¨¡å‹çš„è®¾å¤‡
    device = next(model.parameters()).device
    
    # æ£€æŸ¥ç½‘ç»œç±»å‹ï¼Œå¦‚æœæ˜¯ResNetï¼Œç»™å‡ºè­¦å‘Š
    if 'resnet' in str(type(model)).lower():
        logging.warning("âš ï¸ æ£€æµ‹åˆ°ResNetæ¨¡å‹ã€‚ResNetä¸­çš„æ®‹å·®è¿æ¥è¦æ±‚è¾“å…¥/è¾“å‡ºé€šé“æ•°åŒ¹é…ã€‚")
        logging.warning("âš ï¸ æœ¬å®ç°å¯èƒ½ä¸é€‚ç”¨äºResNetã€‚å»ºè®®è·³è¿‡é‡å»ºæ­¥éª¤æˆ–ä½¿ç”¨ä¸“ä¸šçš„å‰ªæåº“ã€‚")
        logging.warning("âš ï¸ ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°å±‚é—´é€šé“ä¸åŒ¹é…çš„é”™è¯¯ã€‚")
    
    # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
    new_model = copy.deepcopy(model)
    
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆå®ç°ï¼Œé€‚ç”¨äºç®€å•ç½‘ç»œ
    # å¯¹äºå¤æ‚ç½‘ç»œï¼Œåº”è¯¥ä½¿ç”¨åƒtorch_pruningè¿™æ ·çš„åº“æ¥ä¿æŒä¾èµ–å…³ç³»ä¸€è‡´
    
    # é¦–å…ˆè·å–æ‰€æœ‰Conv2då±‚çš„é›¶é€šé“æ©ç 
    zero_masks = {}
    for name, module in new_model.named_modules():
        if isinstance(module, nn.Conv2d):
            weight = module.weight.data
            out_channels = weight.shape[0]
            
            # æ‰¾å‡ºé›¶é€šé“
            nonzero_mask = []
            for i in range(out_channels):
                channel_norm = torch.sum(torch.abs(weight[i]))
                nonzero_mask.append(channel_norm > 0)
            
            nonzero_mask = torch.tensor(nonzero_mask, dtype=torch.bool, device=device)
            nonzero_count = torch.sum(nonzero_mask).item()
            
            # å¦‚æœæœ‰é€šé“å¯ä»¥ç§»é™¤
            if nonzero_count < out_channels and nonzero_count > 0:
                zero_masks[name] = (nonzero_mask, nonzero_count)
    
    # åˆ›å»ºå·ç§¯å±‚åç§°åˆ°å…¶åçš„BatchNormå±‚çš„æ˜ å°„
    conv_to_bn = {}
    prev_name = None
    
    # é€šè¿‡æ¨¡å‹çš„å‘½åæ¨¡å—æ‰¾å‡ºå·ç§¯å±‚ä¸å…¶åçš„BatchNormå±‚
    for name, module in new_model.named_modules():
        if isinstance(module, nn.BatchNorm2d) and prev_name is not None and prev_name in zero_masks:
            conv_to_bn[prev_name] = name
        
        if isinstance(module, nn.Conv2d):
            prev_name = name
        else:
            prev_name = None
    
    # éå†å¹¶ä¿®æ”¹å·ç§¯å±‚å’Œç›¸åº”çš„BatchNormå±‚
    for conv_name, (nonzero_mask, nonzero_count) in zero_masks.items():
        # è·å–å·ç§¯å±‚
        conv_path = conv_name.split('.')
        conv_module = new_model
        for part in conv_path:
            conv_module = getattr(conv_module, part)
        
        # å¯¹äºResNetï¼Œæˆ‘ä»¬åªå¤„ç†æŸäº›å®‰å…¨çš„å±‚ï¼Œè·³è¿‡å¯èƒ½ç ´åç»“æ„çš„å±‚
        if 'resnet' in str(type(model)).lower():
            # è·³è¿‡ä¸‹é‡‡æ ·å±‚(downsample)å’Œå¯èƒ½å½±å“æ®‹å·®è¿æ¥çš„å±‚
            if 'downsample' in conv_name or conv_name.endswith('conv1') or conv_name.endswith('conv3'):
                logging.warning(f"âš ï¸ è·³è¿‡ResNetå…³é”®å±‚ {conv_name} ä»¥ä¿æŒç½‘ç»œç»“æ„")
                continue
        
        logging.info(f"ğŸ” å±‚ {conv_name}: å‘ç° {conv_module.out_channels - nonzero_count}/{conv_module.out_channels} ä¸ªé›¶é€šé“")
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚ï¼Œè¾“å‡ºé€šé“å‡å°‘
        try:
            # è·å–å‚æ•°
            in_channels = conv_module.in_channels
            out_channels = nonzero_count
            kernel_size = conv_module.kernel_size
            stride = conv_module.stride
            padding = conv_module.padding
            dilation = conv_module.dilation
            groups = conv_module.groups  # ä¿ç•™åŸå§‹åˆ†ç»„
            bias = conv_module.bias is not None
            
            # åˆ›å»ºæ–°çš„å·ç§¯å±‚å¹¶ç§»è‡³ç›¸åŒè®¾å¤‡
            new_conv = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                dilation=dilation,
                groups=groups if groups==1 else max(1, out_channels//in_channels*in_channels),
                bias=bias
            ).to(device)  # ç§»è‡³ä¸åŸæ¨¡å‹ç›¸åŒçš„è®¾å¤‡
            
            # å¤åˆ¶éé›¶é€šé“çš„æƒé‡
            idx = 0
            for i in range(conv_module.out_channels):
                if nonzero_mask[i]:
                    new_conv.weight.data[idx] = conv_module.weight.data[i]
                    # é¢å¤–æ£€æŸ¥ç¡®ä¿biaså­˜åœ¨
                    if bias and conv_module.bias is not None and new_conv.bias is not None:
                        new_conv.bias.data[idx] = conv_module.bias.data[i]
                    idx += 1
            
            # æ›¿æ¢å·ç§¯å±‚
            parent_name = '.'.join(conv_path[:-1])
            attr_name = conv_path[-1]
            
            if parent_name:
                parent = new_model
                for part in parent_name.split('.'):
                    parent = getattr(parent, part)
                setattr(parent, attr_name, new_conv)
            else:
                setattr(new_model, attr_name, new_conv)
            
            logging.info(f"âœ… æˆåŠŸæ›¿æ¢å·ç§¯å±‚ {conv_name}ï¼Œè¾“å‡ºé€šé“ä» {conv_module.out_channels} å‡å°‘åˆ° {nonzero_count}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åº”çš„BatchNormå±‚éœ€è¦æ›´æ–°
            if conv_name in conv_to_bn:
                bn_name = conv_to_bn[conv_name]
                bn_path = bn_name.split('.')
                bn_module = new_model
                for part in bn_path:
                    bn_module = getattr(bn_module, part)
                
                # åˆ›å»ºæ–°çš„BatchNormå±‚
                new_bn = nn.BatchNorm2d(
                    num_features=nonzero_count,
                    eps=bn_module.eps,
                    momentum=bn_module.momentum,
                    affine=bn_module.affine,
                    track_running_stats=bn_module.track_running_stats
                ).to(device)
                
                # å¤åˆ¶éé›¶é€šé“çš„å‚æ•°
                if bn_module.affine:
                    idx = 0
                    for i in range(bn_module.num_features):
                        if nonzero_mask[i]:
                            new_bn.weight.data[idx] = bn_module.weight.data[i]
                            new_bn.bias.data[idx] = bn_module.bias.data[i]
                            idx += 1
                
                # å¤åˆ¶è¿è¡Œæ—¶ç»Ÿè®¡æ•°æ®
                if bn_module.track_running_stats:
                    idx = 0
                    for i in range(bn_module.num_features):
                        if nonzero_mask[i]:
                            # ç¡®ä¿ä¸è®¿é—®None
                            if hasattr(bn_module, 'running_mean') and bn_module.running_mean is not None and \
                               hasattr(new_bn, 'running_mean') and new_bn.running_mean is not None:
                                new_bn.running_mean[idx] = bn_module.running_mean[i]
                            
                            if hasattr(bn_module, 'running_var') and bn_module.running_var is not None and \
                               hasattr(new_bn, 'running_var') and new_bn.running_var is not None:
                                new_bn.running_var[idx] = bn_module.running_var[i]
                            
                            idx += 1
                
                # æ›¿æ¢BatchNormå±‚
                parent_name = '.'.join(bn_path[:-1])
                attr_name = bn_path[-1]
                
                if parent_name:
                    parent = new_model
                    for part in parent_name.split('.'):
                        parent = getattr(parent, part)
                    setattr(parent, attr_name, new_bn)
                else:
                    setattr(new_model, attr_name, new_bn)
                
                logging.info(f"âœ… æˆåŠŸæ›¿æ¢BatchNormå±‚ {bn_name}ï¼Œç‰¹å¾æ•°ä» {bn_module.num_features} å‡å°‘åˆ° {nonzero_count}")
                
        except Exception as e:
            logging.warning(f"âš ï¸ å¤„ç†å±‚ {conv_name} æ—¶å‡ºé”™: {e}")
    
    # è®¡ç®—å‚æ•°æ•°é‡ä»¥éªŒè¯ç»“æ„æ˜¯å¦çœŸçš„å˜å°
    orig_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    new_params = sum(p.numel() for p in new_model.parameters() if p.requires_grad)
    reduction = (orig_params - new_params) / orig_params * 100 if orig_params > 0 else 0
    
    logging.info(f"ğŸ“Š å‚æ•°: {orig_params:,} â†’ {new_params:,} (å‡å°‘äº† {reduction:.2f}%)")
    
    # ç¡®ä¿æ•´ä¸ªæ¨¡å‹éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    new_model = new_model.to(device)
    
    # å°è¯•è¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ä»¥éªŒè¯æ¨¡å‹ç»“æ„
    try:
        dummy_input = torch.randn(1, 3, 32, 32).to(device)  # ç¤ºä¾‹è¾“å…¥ï¼Œæ ¹æ®æ¨¡å‹è°ƒæ•´å°ºå¯¸
        new_model.eval()
        with torch.no_grad():
            _ = new_model(dummy_input)
        logging.info("âœ… é‡å»ºåçš„æ¨¡å‹éªŒè¯æˆåŠŸï¼Œç»“æ„ä¸€è‡´")
    except Exception as e:
        logging.error(f"âŒ æ¨¡å‹ç»“æ„ä¸ä¸€è‡´ï¼Œå¯èƒ½éœ€è¦ä¸“ä¸šçš„ç»“æ„åŒ–å‰ªæå·¥å…·: {e}")
        # ç”±äºæˆ‘ä»¬ä¸å¸Œæœ›ä¸­æ–­æ‰§è¡Œæµç¨‹ï¼Œè¿™é‡Œè¿”å›åŸå§‹æ¨¡å‹
        logging.warning("âš ï¸ è¿”å›åŸå§‹æ¨¡å‹ï¼ˆå¸¦é›¶æƒé‡ï¼‰")
        return model
    
    return new_model

def prune_model_properly(model, pruning_ratio=0.5, dataset='cifar10'):
    try:
        import torch_pruning as tp
        
        logging.info("ğŸ”ª å¼€å§‹ç»“æ„åŒ–å‰ªæï¼Œä½¿ç”¨torch_pruningåº“...")
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        device = next(model.parameters()).device
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        if dataset == 'imagenet':
            example_inputs = torch.randn(1, 3, 224, 224).to(device)
        else:
            example_inputs = torch.randn(1, 3, 32, 32).to(device)
            
        # åˆ›å»ºå‰ªæå™¨ - ä½¿ç”¨2.xç‰ˆæœ¬API
        importance = tp.importance.MagnitudeImportance(p=1)  # L1èŒƒæ•°
        
        # ä¿æŠ¤åˆ†ç±»å±‚
        ignored_layers = []
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                ignored_layers.append(module)
                logging.info(f"ğŸ›¡ï¸ ä¿æŠ¤åˆ†ç±»å±‚ {name}")
                
        # åˆ›å»ºå‰ªæå™¨
        pruner = tp.pruner.MagnitudePruner(
            model,
            example_inputs, 
            importance,
            ch_sparsity=pruning_ratio,  # é€šé“ç¨€ç–åº¦
            ignored_layers=ignored_layers,  # ä¿æŠ¤å±‚
        )
        
        # æ‰§è¡Œå‰ªæ
        pruner.step()
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
        model.eval()
        with torch.no_grad():
            output = model(example_inputs)
            logging.info(f"âœ… å‰ªæåæ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
        # è®¡ç®—å‚æ•°æ•°é‡
        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logging.info(f"ğŸ“Š å‰ªæåçš„å‚æ•°æ•°é‡: {param_count:,}")
        
        return model
        
    except Exception as e:
        logging.error(f"âŒ ç»“æ„åŒ–å‰ªæå¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return model

if __name__ == "__main__":
    args = parse_args()
    results, model = run_experiment(args) 