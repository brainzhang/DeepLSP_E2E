#!/usr/bin/env python3
# pyright: reportMissingImports=false, reportCallIssue=false, reportArgumentType=false

import os
import sys
import argparse
import logging
import torch
import json
from datetime import datetime
from pathlib import Path
import torch.onnx
import torch.nn as nn
import copy
import numpy as np
import time
from typing import Tuple, Dict, Any

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import modules from the project
from config.config import MODEL_MAPPING, get_input_size, get_experiment_dir
from utils.logging_setup import setup_logging, log_system_info
from data.datasets import get_dataloaders
from models.model_factory import create_model, fix_resnet_residual_blocks

# Import specific functions from modules
from core.training import train_model, fine_tune_model, evaluate_model
from core.pruning import StructuredPruning
from core.pruning_metrics import calculate_pruning_metrics
from utils.metrics import count_parameters, calculate_flops, measure_inference_time, optimize_model_for_inference
from utils.visualization import plot_training_history, save_results

# Ëøô‰∫õÂèØËÉΩÈúÄË¶ÅÂçïÁã¨ÂÆâË£Ö
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    logging.warning("‚ö†Ô∏è ONNXRuntime not available. Install with: pip install onnxruntime-gpu")

try:
    import torch_pruning as tp
    from torch_pruning import MetaPruner
    TORCH_PRUNING_AVAILABLE = True
except ImportError:
    TORCH_PRUNING_AVAILABLE = False
    logging.warning("‚ö†Ô∏è torch_pruning not available. Install with: pip install torch-pruning")

# Â∞ùËØïÂØºÂÖ•TensorRT
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    logging.warning("‚ö†Ô∏è TensorRT not available. Follow https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html to install.")

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Run structured pruning experiments')
    
    # Basic experiment settings

    parser.add_argument('model', type=str, default='resnet18',
                        help='Model architecture to use (defaults to dataset default)')
    parser.add_argument('dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100', 'imagenet', 'fashionmnist'],
                        help='Dataset to use for experiments')
    parser.add_argument('--output-dir', type=str, default='/mnt/d/Ubuntu/phd/Prune_Methods/Restructured_SP/experiments',
                        help='Directory to save experiment results')
    
    # Training settings
    parser.add_argument('--base-train-epochs', type=int, default=100,
                        help='Number of epochs for base training')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                        help='Learning rate for training')
    parser.add_argument('--batch-size', type=int, default=128,
                        help='Batch size for training and evaluation')
    parser.add_argument('--weight-decay', type=float, default=5e-4,
                        help='Weight decay for training')
    parser.add_argument('--patience', type=int, default=20,
                        help='Early stopping patience')
    
    # Pruning settings
    parser.add_argument('--pruning-method', nargs='+', 
                        default=['channel', 'kernel', 'intra-kernel'],
                        choices=['channel', 'kernel', 'intra-kernel'],
                        help='Methods for pruning (can specify multiple methods)')
    parser.add_argument('--pruning-ratio', type=float, default=0.5,
                        help='Pruning ratio (fraction of channels to remove)')
    parser.add_argument('--global-pruning', action='store_true',
                        help='Use global pruning across all layers (default: layer-wise)')
    
    # Fine-tuning settings
    parser.add_argument('--finetune-epochs', type=int, default=10,
                        help='Number of epochs for fine-tuning after pruning')
    parser.add_argument('--finetune-lr', type=float, default=0.001,
                        help='Learning rate for fine-tuning')
    
    # Execution settings
    parser.add_argument('--no-cuda', action='store_true',
                        help='Disable CUDA even if available')
    parser.add_argument('--no-fp16', action='store_true',
                        help='Disable mixed precision training')
    parser.add_argument('--num-workers', type=int, default=4,
                        help='Number of worker threads for data loading')
    parser.add_argument('--resume', action='store_true',
                        help='Resume training from checkpoint if available')
    parser.add_argument('--skip-base-training', action='store_true',
                        help='Skip base training and start from a pretrained model')
    parser.add_argument('--skip-pruning', action='store_true',
                        help='Skip pruning step (for baseline evaluations)')
    parser.add_argument('--eval-only', action='store_true',
                        help='Only run evaluation on an existing model')
    
    # Êé®ÁêÜ‰ºòÂåñÈÄâÈ°π
    parser.add_argument('--use-tensorrt', action='store_true',
                        help='Use TensorRT for optimized inference')
    parser.add_argument('--use-onnx', action='store_true',
                        help='Use ONNX Runtime for optimized inference')
    parser.add_argument('--align-channels', action='store_true',
                        help='Align pruned channels to hardware-friendly values (e.g., multiples of 32)')
    parser.add_argument('--hardware-align', type=int, default=32,
                        help='Channel alignment value for hardware-friendly pruning (default: 32)')
    parser.add_argument('--skip-rebuild', action='store_true',
                        help='Skip rebuilding the model structure after pruning (keep zero weights)')
    
    # Model loading/saving
    parser.add_argument('--save-model', action='store_true',
                        help='Save models during training')
    parser.add_argument('--model-path', type=str, default=None,
                        help='Path to a saved model to load')
    
    return parser.parse_args()

def run_experiment(args):
    """
    Run a complete pruning experiment based on provided arguments.
    
    Args:
        args: Command line arguments
    """
    # Set up timestamp for this experiment
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Select model architecture based on dataset if not specified
    if args.model is None:
        model_name = MODEL_MAPPING[args.dataset]['default']
    else:
        model_name = args.model
    
    # Create experiment directory and setup logging
    experiment_dir = get_experiment_dir(args.dataset, model_name, args.pruning_method, timestamp)
    log_path = setup_logging(args.dataset, model_name, timestamp)
    
    # Log experiment configuration
    logging.info(f"üß™ Starting experiment with {model_name} on {args.dataset}")
    logging.info(f"üß™ Pruning methods: {', '.join(args.pruning_method)}, ratio: {args.pruning_ratio}")
    
    # ÂàùÂßãÂåñÂÖ≥ÈîÆÂèòÈáèÔºåÈÅøÂÖçÊú™ÁªëÂÆöÈîôËØØ
    pruned_model = None
    
    # Get device
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device, cuda_available = log_system_info()
    
    if not use_cuda:
        device = torch.device('cpu')
        logging.info("Using CPU for computation (CUDA disabled or not available)")
    
    # Set random seed for reproducibility
    torch.manual_seed(0)
    if cuda_available:
        torch.cuda.manual_seed_all(0)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # Load dataset
    logging.info(f"üìö Loading {args.dataset} dataset")
    data = get_dataloaders(
        dataset_name=args.dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    train_loader = data['train_loader']
    test_loader = data['test_loader']
    num_classes = data['num_classes']
    input_size = data['input_size']
    
    # Create results dictionary to store experiment metrics
    results = {
        'dataset': args.dataset,
        'model': model_name,
        'pruning_method': args.pruning_method,
        'pruning_ratio': args.pruning_ratio,
        'global_pruning': args.global_pruning,
        'timestamp': timestamp,
    }
    
    # Create and train the model (if not evaluating only)
    if not args.eval_only:
        if args.model_path is not None and os.path.exists(args.model_path):
            # Load existing model
            logging.info(f"‚¨áÔ∏è Loading model from {args.model_path}")
            
            # Create model architecture
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=False,
                device=device
            )
            
            # Load saved weights
            model.load_state_dict(torch.load(args.model_path, map_location=device))
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            logging.info(f"‚úÖ Model loaded successfully: {model_name}")
        elif args.skip_base_training:
            # Create model with pretrained weights
            logging.info(f"‚¨áÔ∏è Creating model with pretrained weights: {model_name}")
            
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=True,
                device=device
            )
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            # Perform a quick evaluation before pruning
            orig_loss, orig_acc = evaluate_model(model, test_loader)
            logging.info(f"üìä Pretrained model - Loss: {orig_loss:.4f}, Accuracy: {orig_acc:.2f}%")
            results['pretrained_accuracy'] = orig_acc
        else:
            # Create and train model from scratch
            logging.info(f"üèóÔ∏è Creating and training model from scratch: {model_name}")
            
            model = create_model(
                model_name=model_name,
                dataset=args.dataset,
                num_classes=num_classes,
                pretrained=False,
                device=device
            )
            
            # Fix any potential issues in residual blocks
            fix_resnet_residual_blocks(model, device)
            
            # Train the model
            use_fp16 = not args.no_fp16
            train_results = train_model(
                model=model,
                train_loader=train_loader,
                val_loader=test_loader,
                num_epochs=args.base_train_epochs,
                learning_rate=args.learning_rate,
                weight_decay=args.weight_decay,
                patience=args.patience,
                fp16=use_fp16
            )
            
            # Save training history
            results['base_training'] = {
                'epochs': len(train_results['history']['loss']),
                'best_epoch': train_results['best_epoch'],
                'best_val_acc': train_results['best_val_acc'],
                'training_time': train_results['training_time']
            }
            
            # Plot training history
            plot_path = os.path.join(experiment_dir, f"{model_name}_training_history.png")
            plot_training_history(train_results['history'], plot_path)
            
            # Save the trained model if requested
            if args.save_model:
                model_save_path = os.path.join(experiment_dir, f"{model_name}_base_trained.pth")
                torch.save(model.state_dict(), model_save_path)
                logging.info(f"üíæ Base trained model saved to: {model_save_path}")
        
        # Calculate model size and FLOPs before pruning
        orig_params = count_parameters(model)
        orig_flops = calculate_flops(model, input_size)
        
        # ÂàõÂª∫Ê®°ÂûãÂâØÊú¨Áî®‰∫éÊé®ÁêÜÊµãÈáèÔºå‰øùÁïôÂéüÂßãÊ®°Âûã
        logging.info("üìä ÂáÜÂ§áÊµãÈáèÂéüÂßãÊ®°ÂûãÊé®ÁêÜÊó∂Èó¥...")
        orig_model_copy = copy.deepcopy(model)
        
        # ‰ºòÂåñÊ®°Âûã‰ª•ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶
        logging.info("üöÄ ‰ºòÂåñÂéüÂßãÊ®°Âûã‰ª•ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶...")
        optimized_original = optimize_model_for_inference(orig_model_copy, input_size=input_size)
        
        # ÊµãÈáèÊé®ÁêÜÊó∂Èó¥
        orig_inference_time = measure_inference_time(optimized_original, input_size=input_size, use_jit=True)
        
        results['original_model'] = {
            'parameters': orig_params,
            'flops': orig_flops,
            'inference_time_ms': orig_inference_time
        }
        
        logging.info(f"üìä Original model - Parameters: {orig_params:,}, FLOPs: {orig_flops/1e6:.2f}M, Inference time: {orig_inference_time:.2f}ms")
        
        # Skip pruning if requested
        if not args.skip_pruning:
            # Apply pruning methods
            # ‰ΩøÁî®Ê†áÂáÜÂâ™ÊûùÊñπÊ≥ï
            logging.info(f"‚úÇÔ∏è Applying pruning methods: {', '.join(args.pruning_method)}, ratio: {args.pruning_ratio}")
            
            pruner = StructuredPruning(model, pruning_ratio=args.pruning_ratio)
            pruned_model = model
            
            # Â∫îÁî®Ââ™ÊûùÔºàÊ≠§Êó∂Âè™ÊòØÁΩÆÈõ∂ÊùÉÈáçÔºâ
            for method in args.pruning_method:
                logging.info(f"‚úÇÔ∏è Applying {method} pruning method")
                pruned_model = pruner.prune_model(method=method, global_pruning=args.global_pruning)
            
            # Â¶ÇÊûútorch_pruningÂèØÁî®Ôºå‰ΩøÁî®‰∏ì‰∏öÂ∑•ÂÖ∑ËøõË°åÁªìÊûÑÂåñÂâ™Êûù
            if not args.skip_rebuild and TORCH_PRUNING_AVAILABLE:
                logging.info("üîÑ ‰ΩøÁî®torch_pruning‰∏ì‰∏öÂ∑•ÂÖ∑ËøõË°åÁªìÊûÑÂåñÂâ™Êûù...")
                try:
                    # ‰ΩøÁî®‰∏ì‰∏öÂâ™ÊûùÂ∑•ÂÖ∑ (ÂÖ®Â±ÄÂáΩÊï∞)
                    pruned_model = prune_model_properly(pruned_model, args.pruning_ratio, args.dataset)
                except Exception as e:
                    logging.error(f"‚ùå ‰∏ì‰∏öÂâ™ÊûùÂ∑•ÂÖ∑Ë∞ÉÁî®Â§±Ë¥•: {e}")
                    logging.warning("‚ö†Ô∏è ÂõûÈÄÄÂà∞Âü∫Êú¨Ââ™ÊûùÊñπÊ≥ï")
            
            # ËØÑ‰º∞ÁΩÆÈõ∂ÊùÉÈáçÂêéÁöÑÊ®°Âûã
            masked_loss, masked_acc = evaluate_model(pruned_model, test_loader)
            logging.info(f"üìä Pruned model (with masks) - Loss: {masked_loss:.4f}, Accuracy: {masked_acc:.2f}%")
            
            results['masked_model'] = {
                'accuracy': masked_acc,
                'loss': masked_loss
            }
            
            # Êõ¥Êñ∞Ê®°ÂûãÂºïÁî®
            model = pruned_model
            
            # Fine-tune the pruned model if requested
            if args.finetune_epochs > 0:
                logging.info(f"üîÑ Fine-tuning pruned model for {args.finetune_epochs} epochs")
                
                # Fine-tune with mixed precision if enabled
                use_fp16 = not args.no_fp16
                finetune_results = fine_tune_model(
                    model=model,
                    train_loader=train_loader,
                    val_loader=test_loader,
                    num_epochs=args.finetune_epochs,
                    learning_rate=args.finetune_lr,
                    fp16=use_fp16
                )
                
                # Save fine-tuning history
                results['fine_tuning'] = {
                    'epochs': len(finetune_results['history']['loss']),
                    'best_epoch': finetune_results['best_epoch'],
                    'best_val_acc': finetune_results['best_val_acc'],
                    'training_time': finetune_results['training_time']
                }
                
                # Plot fine-tuning history
                plot_path = os.path.join(experiment_dir, f"{model_name}_finetuning_history.png")
                plot_training_history(finetune_results['history'], plot_path)
                
                # Save the fine-tuned model if requested
                if args.save_model:
                    model_save_path = os.path.join(experiment_dir, f"{model_name}_finetuned.pth")
                    torch.save(model.state_dict(), model_save_path)
                    logging.info(f"üíæ Fine-tuned model saved to: {model_save_path}")
            
            # Rebuild the model to remove pruned channels
            logging.info("üî® Calculating pruning metrics")
            
            # Get metrics without rebuilding the model
            metrics = calculate_pruning_metrics(
                model=model,
                input_size=input_size
            )
            
            # Keep the same model instead of rebuilding
            rebuilt_model = model
            
            # Update model to rebuilt version
            model = rebuilt_model
            
            # Validate the rebuilt model with a single forward pass
            try:
                model.eval()
                with torch.no_grad():
                    dummy_input = torch.randn(1, input_size[1], input_size[2], input_size[3], device=device)
                    _ = model(dummy_input)
                logging.info("‚úÖ Validated rebuilt model with test forward pass")
                
                # Measure inference time after pruning
                logging.info("üìä Measuring inference time after pruning...")
                
                # ‰ΩøÁî®ÂΩìÂâçÊ®°Âûã‰Ωú‰∏∫ËØÑ‰º∞Âü∫ÂáÜ
                model_for_eval = model  # ‰øùÂ≠òÁî®‰∫éËØÑ‰º∞ÁöÑÊ†áÂáÜÊ®°Âûã
                
                # Â¶ÇÊûúÂêØÁî®‰∫ÜÈÄöÈÅìÂØπÈΩê
                if args.align_channels:
                    logging.info(f"üîß ÂØπÈΩêÊ®°ÂûãÈÄöÈÅìÂà∞{args.hardware_align}ÁöÑÂÄçÊï∞...")
                    model = align_pruning_channels(model, align_to=args.hardware_align)
                
                # ‰ºòÂåñÊ®°Âûã‰ª•ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶
                logging.info("üöÄ ‰ºòÂåñÊ®°Âûã‰ª•ÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶...")
                
                # ÂàõÂª∫ONNXÊ®°ÂûãË∑ØÂæÑ
                onnx_path = os.path.join(experiment_dir, f"{model_name}_pruned.onnx")
                
                # Á°Æ‰øùÂÆûÈ™åÁõÆÂΩïÂ≠òÂú®
                os.makedirs(os.path.dirname(onnx_path), exist_ok=True)
                
                # ÊµãÈáèÊé®ÁêÜÊó∂Èó¥
                pruned_inference_time = 0.0
                
                # Ê†πÊçÆÁî®Êà∑ÈÄâÊã©ÁöÑ‰ºòÂåñÊñπÊ≥ïÊµãÈáèÊé®ÁêÜÊó∂Èó¥
                if args.use_tensorrt and TENSORRT_AVAILABLE:
                    logging.info("üöÄ ‰ΩøÁî®TensorRTËøõË°åÊé®ÁêÜ‰ºòÂåñ...")
                    
                    # ÂàõÂª∫TensorRTÂºïÊìé
                    engine_path = os.path.join(experiment_dir, f"{model_name}_pruned.engine")
                    engine = create_tensorrt_engine(
                        model, 
                        input_size=input_size, 
                        onnx_path=onnx_path, 
                        engine_path=engine_path
                    )
                    
                    # ‰ΩøÁî®TensorRTÊµãÈáèÊé®ÁêÜÊó∂Èó¥
                    if engine is not None:
                        pruned_inference_time = measure_tensorrt_inference_time(
                            engine,
                            input_size=input_size
                        )
                    else:
                        logging.warning("‚ö†Ô∏è TensorRTÂºïÊìéÂàõÂª∫Â§±Ë¥•ÔºåÂõûÈÄÄÂà∞PyTorchÊé®ÁêÜ")
                        optimized_model = optimize_model_for_inference(copy.deepcopy(model), input_size=input_size)
                        pruned_inference_time = measure_inference_time(optimized_model, input_size=input_size, use_jit=True)
                
                elif args.use_onnx and ONNX_AVAILABLE:
                    logging.info("üöÄ ‰ΩøÁî®ONNX RuntimeËøõË°åÊé®ÁêÜ‰ºòÂåñ...")
                    
                    # ÂØºÂá∫‰∏∫ONNXÊ†ºÂºè
                    dummy_input = torch.randn(input_size, device=device)
                    torch.onnx.export(
                        model,
                        dummy_input,
                        onnx_path,
                        export_params=True,
                        opset_version=13,
                        do_constant_folding=True,
                        input_names=['input'],
                        output_names=['output'],
                        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
                    )
                    
                    # ‰ΩøÁî®ONNX RuntimeÊµãÈáèÊé®ÁêÜÊó∂Èó¥
                    pruned_inference_time = measure_onnx_inference_time(
                        onnx_path,
                        input_size=input_size,
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                    
                else:
                    # ‰ΩøÁî®PyTorch JIT‰ºòÂåñ
                    optimized_model = optimize_model_for_inference(copy.deepcopy(model), input_size=input_size)
                    pruned_inference_time = measure_inference_time(optimized_model, input_size=input_size, use_jit=True)
                
                metrics['inference_time_ms'] = pruned_inference_time
                
                # Update inference time speedup
                if 'original_params' in metrics and metrics.get('inference_time_ms', 0) > 0:
                    orig_time = results['original_model']['inference_time_ms']
                    time_speedup = 100.0 * (1.0 - pruned_inference_time / orig_time) if orig_time > 0 else 0
                    metrics['inference_speedup'] = time_speedup
                    
                    # Add inference time reduction metric (negative when time increases)
                    time_reduction = 100.0 * (orig_time - pruned_inference_time) / orig_time if orig_time > 0 else 0
                    metrics['inference_time_reduction'] = time_reduction
                    
                    logging.info(f"üìä Êé®ÁêÜÊó∂Èó¥ÂèòÂåñ: {orig_time:.2f}ms ‚Üí {pruned_inference_time:.2f}ms")
                    logging.info(f"üìä Êé®ÁêÜÊó∂Èó¥Âä†ÈÄüÁéá: {time_speedup:.2f}%, Êó∂Èó¥ÂáèÂ∞ëÁéá: {time_reduction:.2f}%")
                
            except Exception as e:
                logging.error(f"‚ùå Error validating rebuilt model: {e}")
                # Revert to original model
                logging.warning("‚ö†Ô∏è Reverting to original model due to rebuilding error")
                model = pruned_model
            
            # Save metrics from pruning
            results['pruning'] = metrics
            
            # Â¶ÇÊûúÂÆûÈôÖÊâßË°å‰∫ÜÁªìÊûÑÈáçÂª∫ÔºåÈáçÊñ∞ËÆ°ÁÆóFLOPs
            if not args.skip_rebuild:
                # ÈáçÊñ∞ËÆ°ÁÆóÂèÇÊï∞ÂíåFLOPs
                new_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                param_reduction = 100.0 * (1.0 - new_params / orig_params) if orig_params > 0 else 0
                
                # ÈáçÊñ∞ËÆ°ÁÆóFLOPs
                new_flops = calculate_flops(model, input_size)
                flops_reduction = 100.0 * (1.0 - new_flops / orig_flops) if orig_flops > 0 else 0
                
                # Êõ¥Êñ∞ÊåáÊ†á
                metrics['pruned_params'] = new_params
                metrics['param_reduction'] = param_reduction
                metrics['pruned_flops'] = new_flops
                metrics['flops_reduction'] = flops_reduction
                
                logging.info(f"üìä ÈáçÂª∫Âêé - ÂèÇÊï∞: {new_params:,} ({param_reduction:.2f}% ÂáèÂ∞ë)")
                logging.info(f"üìä ÈáçÂª∫Âêé - FLOPs: {new_flops/1e6:.2f}M ({flops_reduction:.2f}% ÂáèÂ∞ë)")
            
            # Evaluate final model - Áõ¥Êé•‰ΩøÁî®ÂéüÂßãÊ®°ÂûãËøõË°åËØÑ‰º∞
            final_loss, final_acc = evaluate_model(model, test_loader)
            logging.info(f"üìä Final rebuilt model - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%")
            
            # Use metrics from the pruned model for final model parameters
            results['final_model'] = {
                'accuracy': final_acc,
                'loss': final_loss,
                'parameters': metrics.get('pruned_params', orig_params),
                'flops': metrics.get('pruned_flops', orig_flops),
                'inference_time_ms': metrics.get('inference_time_ms', 0),
                'param_reduction': metrics.get('param_reduction', 0.0),
                'flops_reduction': metrics.get('flops_reduction', 0.0),
                'inference_speedup': metrics.get('inference_speedup', 0.0),
                'inference_time_reduction': metrics.get('inference_time_reduction', 0.0)
            }
            
    else:
        # Evaluation only mode
        if args.model_path is None:
            logging.error("‚ùå Model path must be provided for evaluation-only mode")
            return None, None
        
        if not os.path.exists(args.model_path):
            logging.error(f"‚ùå Model file not found: {args.model_path}")
            return None, None
        
        # Create model architecture
        model = create_model(
            model_name=model_name,
            dataset=args.dataset,
            num_classes=num_classes,
            pretrained=False,
            device=device
        )
        
        # Load saved weights
        model.load_state_dict(torch.load(args.model_path, map_location=device))
        
        # Fix any potential issues in residual blocks
        fix_resnet_residual_blocks(model, device)
        
        logging.info(f"‚úÖ Model loaded successfully: {model_name}")
    
    # Final evaluation
    logging.info("üìä ÊâßË°åÊúÄÁªàËØÑ‰º∞...")
    final_loss, final_acc = evaluate_model(model, test_loader)
    logging.info(f"üìä Final evaluation - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%")
    
    # Do NOT recalculate parameters and FLOPs here - use values from pruning metrics
    if 'final_model' not in results:
        results['final_model'] = {}
    
    # Update only accuracy and loss, keeping pruning metrics intact
    results['final_model'].update({
        'accuracy': final_acc,
        'loss': final_loss
    })
    
    # If we're in evaluation-only mode or skipped pruning, we need to calculate metrics
    if args.eval_only or args.skip_pruning:
        final_params = count_parameters(model)
        final_flops = calculate_flops(model, input_size)
        
        results['final_model'].update({
            'parameters': final_params,
            'flops': final_flops
        })
        
        # Calculate reduction ratios if original model is available
        if 'original_model' in results:
            orig_params = results['original_model']['parameters']
            orig_flops = results['original_model']['flops']
            
            if orig_params > 0:
                param_reduction = 100.0 * (1.0 - final_params / orig_params)
                results['final_model']['param_reduction'] = param_reduction
                
                if orig_flops > 0:
                    flops_reduction = 100.0 * (1.0 - final_flops / orig_flops)
                    results['final_model']['flops_reduction'] = flops_reduction
                    
                    logging.info(f"üìä Model size reduction: {param_reduction:.2f}%, FLOPs reduction: {flops_reduction:.2f}%")
    # For pruned models, use the pruning metrics directly (already stored in results['final_model'] earlier)
    else:
        logging.info(f"üìä Model size reduction: {results['final_model']['param_reduction']:.2f}%, " 
                    f"FLOPs reduction: {results['final_model']['flops_reduction']:.2f}%")
    
    # Save results
    results_path = os.path.join(experiment_dir, f"{model_name}_results.json")
    save_results(results, results_path)
    logging.info(f"üíæ Experiment results saved to: {results_path}")
    
    # Create a summary
    logging.info(f"\n{'='*60}\nüìù EXPERIMENT SUMMARY\n{'='*60}")
    logging.info(f"Model: {model_name}, Dataset: {args.dataset}")
    logging.info(f"Pruning methods: {', '.join(args.pruning_method)}, Ratio: {args.pruning_ratio}")
    
    if 'original_model' in results:
        logging.info(f"Original model - Parameters: {results['original_model']['parameters']:,}, "
                    f"FLOPs: {results['original_model']['flops']/1e6:.2f}M, "
                    f"Inference time: {results['original_model'].get('inference_time_ms', 0):.2f}ms")
    
    if 'final_model' in results:
        logging.info(f"Final model - Parameters: {results['final_model']['parameters']:,}, "
                    f"FLOPs: {results['final_model']['flops']/1e6:.2f}M, "
                    f"Inference time: {results['final_model'].get('inference_time_ms', 0):.2f}ms")
        
        if 'param_reduction' in results['final_model']:
            logging.info(f"Parameter reduction: {results['final_model']['param_reduction']:.2f}%")
        
        if 'flops_reduction' in results['final_model']:
            logging.info(f"FLOPs reduction: {results['final_model']['flops_reduction']:.2f}%")
        
        if 'inference_speedup' in results['final_model']:
            logging.info(f"Inference speedup: {results['final_model']['inference_speedup']:.2f}%")
        
        if 'inference_time_reduction' in results['final_model']:
            logging.info(f"Inference time reduction: {results['final_model']['inference_time_reduction']:.2f}%")
        
        logging.info(f"Final accuracy: {results['final_model']['accuracy']:.2f}%")
    
    logging.info(f"{'='*60}")
    
    return results, model

def create_tensorrt_engine(model, input_size, onnx_path=None, engine_path=None, precision='fp16'):
    """
    Â∞ÜPyTorchÊ®°ÂûãËΩ¨Êç¢‰∏∫TensorRTÂºïÊìé‰ª•Âä†ÈÄüÊé®ÁêÜ
    
    Args:
        model: PyTorchÊ®°Âûã
        input_size: ËæìÂÖ•Â∞∫ÂØ∏ (batch_size, channels, height, width)
        onnx_path: ONNXÊ®°ÂûãË∑ØÂæÑÔºåÂ¶ÇÊûú‰∏çÊèê‰æõÂàôÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂
        engine_path: TensorRTÂºïÊìé‰øùÂ≠òË∑ØÂæÑ
        precision: Á≤æÂ∫¶Ê®°Âºè ('fp32', 'fp16', 'int8')
        
    Returns:
        TensorRTÂºïÊìé
    """
    if not TENSORRT_AVAILABLE:
        logging.error("‚ùå TensorRT not available for optimized inference")
        return None
    
    # Á°Æ‰øùÊ®°ÂûãÂ§Ñ‰∫éËØÑ‰º∞Ê®°Âºè
    model.eval()
    
    # Â¶ÇÊûúÊú™Êèê‰æõONNXË∑ØÂæÑÔºåÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂
    if onnx_path is None:
        import tempfile
        temp_dir = tempfile.gettempdir()
        onnx_path = os.path.join(temp_dir, f"temp_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.onnx")
    
    # Â¶ÇÊûúÊú™Êèê‰æõÂºïÊìéË∑ØÂæÑÔºåÊ†πÊçÆONNXË∑ØÂæÑÂàõÂª∫
    if engine_path is None:
        engine_path = onnx_path.replace(".onnx", ".engine")
    
    # ÂàõÂª∫ÈöèÊú∫ËæìÂÖ•
    device = next(model.parameters()).device
    dummy_input = torch.randn(input_size, device=device)
    
    # ÂÖàÂØºÂá∫‰∏∫ONNXÊ†ºÂºè
    logging.info(f"üì¶ ÂØºÂá∫Ê®°Âûã‰∏∫ONNXÊ†ºÂºè: {onnx_path}")
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=13,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )
    
    # ÂàõÂª∫TensorRTÂºïÊìé
    logging.info(f"üöÄ ÊûÑÂª∫TensorRTÂºïÊìé: {engine_path}")
    
    try:
        # Á°Æ‰øùTensorRTÂ∑≤ÊàêÂäüÂØºÂÖ•
        import tensorrt as trt  # pyright: ignore[reportAttributeAccessIssue]
            
        # ÂàõÂª∫TensorRT builderÂíåÁΩëÁªú
        logger = trt.Logger(trt.Logger.WARNING)  # pyright: ignore[reportAttributeAccessIssue]
        builder = trt.Builder(logger)  # pyright: ignore[reportAttributeAccessIssue]
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))  # pyright: ignore[reportAttributeAccessIssue]
        parser = trt.OnnxParser(network, logger)  # pyright: ignore[reportAttributeAccessIssue]
        
        # Ëß£ÊûêONNXÊñá‰ª∂
        with open(onnx_path, 'rb') as f:
            if not parser.parse(f.read()):
                logging.error("‚ùå Ëß£ÊûêONNXÊñá‰ª∂Â§±Ë¥•")
                for error in range(parser.num_errors):
                    logging.error(f"Error {error}: {parser.get_error(error)}")
                return None
        
        # ÈÖçÁΩÆTensorRT
        config = builder.create_builder_config()
        # Êñ∞ÁâàTensorRT API‰ΩøÁî®set_memory_pool_limit
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # pyright: ignore[reportAttributeAccessIssue]
        
        # ‰∏∫Âä®ÊÄÅÊâπÈáèÂ§ßÂ∞èÂàõÂª∫‰ºòÂåñÈÖçÁΩÆÊñá‰ª∂
        profile = builder.create_optimization_profile()
        # ËÆæÁΩÆÊâπÈáèÂ§ßÂ∞èËåÉÂõ¥: ÊúÄÂ∞è„ÄÅÊúÄ‰ºò„ÄÅÊúÄÂ§ß
        min_batch = 1
        opt_batch = input_size[0]  # ‰ºòÂåñÂ§ßÂ∞è‰∏∫ËæìÂÖ•ÊâπÈáèÂ§ßÂ∞è
        max_batch = input_size[0] * 2  # ÊúÄÂ§ßÊâπÈáèÂ§ßÂ∞èËÆæ‰∏∫ËæìÂÖ•ÁöÑ2ÂÄç
        
        # ÈÖçÁΩÆ'input'Âº†ÈáèÁöÑ‰ºòÂåñÂèÇÊï∞
        profile.set_shape(
            "input",  # ËæìÂÖ•ÂêçÁß∞
            (min_batch, input_size[1], input_size[2], input_size[3]),  # ÊúÄÂ∞èÂ∞∫ÂØ∏
            (opt_batch, input_size[1], input_size[2], input_size[3]),  # ÊúÄ‰ºòÂ∞∫ÂØ∏
            (max_batch, input_size[1], input_size[2], input_size[3])   # ÊúÄÂ§ßÂ∞∫ÂØ∏
        )
        config.add_optimization_profile(profile)
        
        # ËÆæÁΩÆÁ≤æÂ∫¶
        if precision == 'fp16' and builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)  # pyright: ignore[reportAttributeAccessIssue]
            logging.info("‚öôÔ∏è ÂêØÁî®FP16Á≤æÂ∫¶")
        elif precision == 'int8' and builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)  # pyright: ignore[reportAttributeAccessIssue]
            logging.info("‚öôÔ∏è ÂêØÁî®INT8Á≤æÂ∫¶")
        
        # ÊûÑÂª∫ÂºïÊìé
        serialized_engine = builder.build_serialized_network(network, config)
        with open(engine_path, "wb") as f:
            f.write(serialized_engine)
        
        # ÂàõÂª∫ËøêË°åÊó∂ÂíåÂºïÊìé
        runtime = trt.Runtime(logger)  # pyright: ignore[reportAttributeAccessIssue]
        with open(engine_path, "rb") as f:
            engine = runtime.deserialize_cuda_engine(f.read())
            
        logging.info("‚úÖ TensorRTÂºïÊìéÂàõÂª∫ÊàêÂäü")
        return engine
    
    except ImportError as e:
        logging.error(f"‚ùå TensorRTÂØºÂÖ•Â§±Ë¥•: {e}")
        return None
    except Exception as e:
        logging.error(f"‚ùå ÂàõÂª∫TensorRTÂºïÊìéÂ§±Ë¥•: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return None

def measure_tensorrt_inference_time(engine, input_size, num_iterations=100):
    """
    ‰ΩøÁî®TensorRTÂºïÊìéÊµãÈáèÊé®ÁêÜÊó∂Èó¥
    
    Args:
        engine: TensorRTÂºïÊìé
        input_size: ËæìÂÖ•Â∞∫ÂØ∏ (batch_size, channels, height, width)
        num_iterations: ÊµãÈáèËø≠‰ª£Ê¨°Êï∞
        
    Returns:
        float: Âπ≥ÂùáÊé®ÁêÜÊó∂Èó¥ÔºàÊØ´ÁßíÔºâ
    """
    if engine is None:
        logging.error("‚ùå Êó†ÊïàÁöÑTensorRTÂºïÊìé")
        return 0
    
    # Ê£ÄÊü•ÂøÖË¶ÅÁöÑÂ∫ìÊòØÂê¶Â∑≤ÂØºÂÖ•
    if not TENSORRT_AVAILABLE:
        logging.error("‚ùå TensorRTÊú™Ê≠£Á°ÆÂØºÂÖ•")
        return 0
        
    try:
        # ÈÅøÂÖçÈùôÊÄÅÂàÜÊûêÂô®ÁöÑË≠¶ÂëäÔºåÂä®ÊÄÅÂØºÂÖ•ÊâÄÈúÄÊ®°Âùó
        import importlib
        cuda_module = importlib.import_module("pycuda.driver")
        
        # ÂàõÂª∫CUDA‰∏ä‰∏ãÊñá
        context = engine.create_execution_context()
        
        # ÂáÜÂ§áËæìÂÖ•ÂíåËæìÂá∫ÂÜÖÂ≠ò - Áõ¥Êé•‰ΩøÁî®numpyÂàõÂª∫Êï∞ÁªÑÔºåÈÅøÂÖçÁ±ªÂûãÈóÆÈ¢ò
        h_input = np.zeros(input_size, dtype=np.float32)
        for i in range(input_size[0]):
            for c in range(input_size[1]):
                for h in range(input_size[2]):
                    for w in range(input_size[3]):
                        h_input[i,c,h,w] = np.random.random()
        
        # Ëé∑ÂèñËæìÂá∫ÂΩ¢Áä∂ - ‰ΩøÁî®Êñ∞ÁâàTensorRT API
        # ÊóßÁâà: output_shape = engine.get_binding_shape(1)
        # ‰ΩøÁî®contextÊàñengineËé∑ÂèñÁªëÂÆöÁª¥Â∫¶ÁöÑÊõø‰ª£ÊñπÊ≥ï
        binding_idx = 1  # ËæìÂá∫ÁªëÂÆöÁ¥¢Âºï
        if hasattr(engine, 'get_binding_dimensions'):
            # Â∞ùËØï‰ΩøÁî®get_binding_dimensions
            output_dims = engine.get_binding_dimensions(binding_idx)
            output_shape = (input_size[0], output_dims[1])  # ÂÅáËÆæ‰∏∫(N, C)Ê†ºÂºè
        elif hasattr(context, 'get_binding_shape'):
            # Â∞ùËØï‰ªé‰∏ä‰∏ãÊñáËé∑Âèñ
            output_shape = context.get_binding_shape(binding_idx)
        else:
            # ÊúÄÂêéÁöÑÂ§áÈÄâÊñπÊ°à - ÂÅáËÆæËæìÂá∫‰∏éÊ®°ÂûãÂØπÂ∫î
            logging.warning("‚ö†Ô∏è Êó†Ê≥ïËé∑ÂèñËæìÂá∫ÂΩ¢Áä∂Ôºå‰ΩøÁî®ÈªòËÆ§ÂÄº(N, 1000)")
            output_shape = (input_size[0], 1000)  # ÂÅáËÆæ‰∏∫1000Á±ªÂàÜÁ±ª
        
        logging.info(f"üìä TensorRTËæìÂá∫ÂΩ¢Áä∂: {output_shape}")
        h_output = np.zeros((input_size[0], output_shape[1]), dtype=np.float32)
        
        # ‰ΩøÁî®getattrÂä®ÊÄÅËé∑ÂèñPyCUDAÂáΩÊï∞ÔºåÈÅøÂÖçÈùôÊÄÅÂàÜÊûêÂô®Ë≠¶Âëä
        try:
            # ÂàÜÈÖçGPUÂÜÖÂ≠ò
            mem_alloc_fn = getattr(cuda_module, "mem_alloc")
            d_input = mem_alloc_fn(h_input.nbytes)
            d_output = mem_alloc_fn(h_output.nbytes)
            
            # ÂàõÂª∫CUDAÊµÅ
            stream_cls = getattr(cuda_module, "Stream")
            stream = stream_cls()
            
            # ÂÜÖÂ≠òÊã∑Ë¥ùÂáΩÊï∞
            htod_async_fn = getattr(cuda_module, "memcpy_htod_async")
            dtoh_async_fn = getattr(cuda_module, "memcpy_dtoh_async")
            
            # È¢ÑÁÉ≠
            for _ in range(10):
                htod_async_fn(d_input, h_input, stream)
                context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
                dtoh_async_fn(h_output, d_output, stream)
                stream.synchronize()
            
            # ÊµãÈáèÊó∂Èó¥
            start_time = time.time()
            for _ in range(num_iterations):
                htod_async_fn(d_input, h_input, stream)
                context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
                dtoh_async_fn(h_output, d_output, stream)
                stream.synchronize()
        except (AttributeError, ImportError) as e:
            # Â¶ÇÊûúÊâæ‰∏çÂà∞ÁâπÂÆöÊñπÊ≥ïÊàñÂØºÂÖ•Â§±Ë¥•ÔºåÂ∞ùËØï‰ΩøÁî®ÂêåÊ≠•ÁâàÊú¨
            logging.warning(f"‚ö†Ô∏è ‰ΩøÁî®ÂêåÊ≠•Êé®ÁêÜÊé•Âè£: {e}")
            
            # ÁÆÄÂåñÁöÑÂêåÊ≠•Êé®ÁêÜÁâàÊú¨
            import pycuda.driver
            
            # ‰ΩøÁî®ÂêåÊ≠•ÁâàÊú¨ÁöÑÊé•Âè£ÔºåÊ∑ªÂä†type:ignoreÊ≥®ÈáäÂøΩÁï•ÈùôÊÄÅÂàÜÊûêÈîôËØØ
            d_input = pycuda.driver.mem_alloc(h_input.nbytes)  # type: ignore
            d_output = pycuda.driver.mem_alloc(h_output.nbytes)  # type: ignore
            
            # È¢ÑÁÉ≠
            for _ in range(10):
                pycuda.driver.memcpy_htod(d_input, h_input)  # type: ignore
                context.execute_v2(bindings=[int(d_input), int(d_output)])
                pycuda.driver.memcpy_dtoh(h_output, d_output)  # type: ignore
            
            # ÊµãÈáèÊó∂Èó¥
            start_time = time.time()
            for _ in range(num_iterations):
                pycuda.driver.memcpy_htod(d_input, h_input)  # type: ignore
                context.execute_v2(bindings=[int(d_input), int(d_output)])
                pycuda.driver.memcpy_dtoh(h_output, d_output)  # type: ignore
        
        elapsed_time = time.time() - start_time
        avg_time_ms = (elapsed_time / num_iterations) * 1000  # ËΩ¨Êç¢‰∏∫ÊØ´Áßí
        
        logging.info(f"üìä TensorRTÂπ≥ÂùáÊé®ÁêÜÊó∂Èó¥: {avg_time_ms:.2f} ms ({num_iterations} Ê¨°Ëø≠‰ª£)")
        
        # ÈáäÊîæËµÑÊ∫ê
        del context
        
        return avg_time_ms
    except ImportError as e:
        logging.error(f"‚ùå PyCUDAÂØºÂÖ•Â§±Ë¥•: {e}")
        return 0
    except Exception as e:
        logging.error(f"‚ùå TensorRTÊé®ÁêÜÊµãÈáèÂ§±Ë¥•: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return 0

def measure_onnx_inference_time(onnx_path, input_size, num_iterations=100, device='cuda'):
    """
    ‰ΩøÁî®ONNX RuntimeÊµãÈáèÊé®ÁêÜÊó∂Èó¥
    
    Args:
        onnx_path: ONNXÊ®°ÂûãË∑ØÂæÑ
        input_size: ËæìÂÖ•Â∞∫ÂØ∏ (batch_size, channels, height, width)
        num_iterations: ÊµãÈáèËø≠‰ª£Ê¨°Êï∞
        device: ËøêË°åËÆæÂ§á ('cuda' Êàñ 'cpu')
        
    Returns:
        float: Âπ≥ÂùáÊé®ÁêÜÊó∂Èó¥ÔºàÊØ´ÁßíÔºâ
    """
    if not ONNX_AVAILABLE:
        logging.error("‚ùå ONNX Runtime not available for optimized inference")
        return 0
    
    if not os.path.exists(onnx_path):
        logging.error(f"‚ùå ONNXÊ®°Âûã‰∏çÂ≠òÂú®: {onnx_path}")
        return 0
    
    # ÂàõÂª∫ONNX‰ºöËØù
    logging.info(f"üì¶ Âä†ËΩΩONNXÊ®°Âûã: {onnx_path}")
    try:
        import onnxruntime as ort
            
        if device == 'cuda':
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        else:
            providers = ['CPUExecutionProvider']
            
        session = ort.InferenceSession(onnx_path, providers=providers)
        
        # ÂáÜÂ§áËæìÂÖ• - Áõ¥Êé•ÂàõÂª∫numpyÊï∞ÁªÑ
        input_name = session.get_inputs()[0].name
        dummy_input = np.zeros(input_size, dtype=np.float32)
        for i in range(input_size[0]):
            for c in range(input_size[1]):
                for h in range(input_size[2]):
                    for w in range(input_size[3]):
                        dummy_input[i,c,h,w] = np.random.random()
        
        # È¢ÑÁÉ≠
        for _ in range(10):
            _ = session.run(None, {input_name: dummy_input})
        
        # ÊµãÈáèÊó∂Èó¥
        start_time = time.time()
        for _ in range(num_iterations):
            _ = session.run(None, {input_name: dummy_input})
        
        elapsed_time = time.time() - start_time
        avg_time_ms = (elapsed_time / num_iterations) * 1000  # ËΩ¨Êç¢‰∏∫ÊØ´Áßí
        
        logging.info(f"üìä ONNX RuntimeÂπ≥ÂùáÊé®ÁêÜÊó∂Èó¥: {avg_time_ms:.2f} ms ({num_iterations} Ê¨°Ëø≠‰ª£)")
        
        return avg_time_ms
    
    except ImportError as e:
        logging.error(f"‚ùå ONNX RuntimeÂØºÂÖ•Â§±Ë¥•: {e}")
        return 0
    except Exception as e:
        logging.error(f"‚ùå ONNX RuntimeÊé®ÁêÜÂ§±Ë¥•: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return 0

# Ê∑ªÂä†Êñ∞ÁöÑÈÄöÈÅìÂØπÈΩêÂâ™ÊûùÂäüËÉΩ
def round_to_multiple(number, multiple):
    """Â∞ÜÊï∞Â≠óÂõõËàç‰∫îÂÖ•Âà∞ÊúÄÊé•ËøëÁöÑÂÄçÊï∞"""
    return multiple * round(number / multiple)

def align_pruning_channels(model, align_to=32):
    """
    Ë∞ÉÊï¥Ê®°ÂûãÂêÑÂ±ÇÁöÑËæìÂá∫ÈÄöÈÅìÊï∞Ôºå‰ΩøÂÖ∂ÂØπÈΩêÂà∞ÊåáÂÆöÁöÑÂÄçÊï∞
    ËøôÂèØ‰ª•Â§ßÂπÖÊèêÈ´òGPUÊé®ÁêÜÊÄßËÉΩ
    
    Args:
        model: PyTorchÊ®°Âûã
        align_to: ÂØπÈΩêÁöÑÈÄöÈÅìÊï∞ÂÄçÊï∞
        
    Returns:
        Ë∞ÉÊï¥ÂêéÁöÑÊ®°Âûã
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # Ëé∑ÂèñÂΩìÂâçÈÄöÈÅìÊï∞
            out_channels = module.out_channels
            # Âêë‰∏ãÂØπÈΩêÂà∞ÊúÄËøëÁöÑÂÄçÊï∞
            aligned_channels = (out_channels // align_to) * align_to
            # Â¶ÇÊûúÂØπÈΩê‰ºöÂáèÂ∞ëÂ§™Â§öÈÄöÈÅìÔºåÂàôÂêë‰∏äÂØπÈΩê
            if out_channels - aligned_channels > align_to / 2:
                aligned_channels += align_to
            
            # Â¶ÇÊûúÈúÄË¶ÅË∞ÉÊï¥‰∏î‰∏ç‰ºö‰ΩøÈÄöÈÅìÊï∞Âèò‰∏∫0
            if aligned_channels != out_channels and aligned_channels > 0:
                logging.info(f"‚öôÔ∏è ÂØπÈΩêÂ±Ç {name} ÁöÑËæìÂá∫ÈÄöÈÅì: {out_channels} -> {aligned_channels}")
                
                # ÂàõÂª∫Êñ∞ÁöÑÂç∑ÁßØÂ±Ç
                aligned_conv = nn.Conv2d(
                    module.in_channels,
                    aligned_channels,
                    module.kernel_size,
                    stride=module.stride,
                    padding=module.padding,
                    dilation=module.dilation,
                    groups=module.groups,
                    bias=module.bias is not None,
                    padding_mode=module.padding_mode
                )
                
                # Â§çÂà∂ÂéüÂßãÊùÉÈáç
                with torch.no_grad():
                    if aligned_channels > out_channels:
                        # Êâ©Â±ïÈÄöÈÅì
                        aligned_conv.weight[:out_channels] = module.weight
                        # Â§ÑÁêÜbiasÔºåÈÅøÂÖçÂØπNone‰ΩøÁî®‰∏ãÊ†á
                        if module.bias is not None and aligned_conv.bias is not None:
                            aligned_conv.bias[:out_channels] = module.bias
                    else:
                        # Êî∂Áº©ÈÄöÈÅì
                        aligned_conv.weight = nn.Parameter(module.weight[:aligned_channels])
                        # Â§ÑÁêÜbiasÔºåÈÅøÂÖçÂØπNone‰ΩøÁî®‰∏ãÊ†á
                        if module.bias is not None and aligned_conv.bias is not None:
                            aligned_conv.bias = nn.Parameter(module.bias[:aligned_channels])
                
                # ÊõøÊç¢Ê®°Âûã‰∏≠ÁöÑÂ±Ç
                # Ê≥®ÊÑèÔºöËøôÁßçÁõ¥Êé•ÊõøÊç¢ÂèØËÉΩ‰ºöÂØºËá¥ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®Â§çÊùÇÁΩëÁªú‰∏≠
                # Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠Â∫îËØ•‰ΩøÁî®Êõ¥ÂÆåÂñÑÁöÑÂ±ÇÊõøÊç¢ÊñπÊ≥ï
                setattr(model, name.split('.')[-1], aligned_conv)
    
    return model

def rebuild_pruned_model(model):
    """
    ÂàõÂª∫ÁªìÊûÑÂåñÁöÑÂâ™ÊûùÊ®°ÂûãÔºåÁúüÊ≠£ÁßªÈô§Èõ∂ÊùÉÈáçÈÄöÈÅìÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂ∞ÜÂÆÉ‰ª¨ÁΩÆ‰∏∫Èõ∂„ÄÇ
    ÂÆûÁé∞ÊñπÂºèÊòØÊâæÂá∫ÈùûÈõ∂ÈÄöÈÅìÔºåÂàõÂª∫Êñ∞ÁöÑÊõ¥Â∞èÁöÑÁΩëÁªú„ÄÇ
    ÂêåÊó∂Â§ÑÁêÜÂêéÁª≠ÁöÑBatchNormÂ±ÇÔºåÁ°Æ‰øùÁª¥Â∫¶ÂåπÈÖç„ÄÇ
    
    Args:
        model: Â∑≤ÁªèÂ∫îÁî®‰∫ÜÊùÉÈáçÊé©Á†ÅÁöÑÊ®°Âûã
        
    Returns:
        new_model: ÁªìÊûÑÊõ¥Â∞èÁöÑÊñ∞Ê®°Âûã
    """
    logging.info("‚öôÔ∏è Ê≠£Âú®ÈáçÂª∫Ê®°ÂûãÔºåÁßªÈô§Èõ∂ÊùÉÈáçÈÄöÈÅì...")
    
    # Ëé∑ÂèñÂéüÂßãÊ®°ÂûãÁöÑËÆæÂ§á
    device = next(model.parameters()).device
    
    # Ê£ÄÊü•ÁΩëÁªúÁ±ªÂûãÔºåÂ¶ÇÊûúÊòØResNetÔºåÁªôÂá∫Ë≠¶Âëä
    if 'resnet' in str(type(model)).lower():
        logging.warning("‚ö†Ô∏è Ê£ÄÊµãÂà∞ResNetÊ®°Âûã„ÄÇResNet‰∏≠ÁöÑÊÆãÂ∑ÆËøûÊé•Ë¶ÅÊ±ÇËæìÂÖ•/ËæìÂá∫ÈÄöÈÅìÊï∞ÂåπÈÖç„ÄÇ")
        logging.warning("‚ö†Ô∏è Êú¨ÂÆûÁé∞ÂèØËÉΩ‰∏çÈÄÇÁî®‰∫éResNet„ÄÇÂª∫ËÆÆË∑≥ËøáÈáçÂª∫Ê≠•È™§Êàñ‰ΩøÁî®‰∏ì‰∏öÁöÑÂâ™ÊûùÂ∫ì„ÄÇ")
        logging.warning("‚ö†Ô∏è ÁªßÁª≠ÊâßË°åÔºå‰ΩÜÂèØËÉΩ‰ºöÂá∫Áé∞Â±ÇÈó¥ÈÄöÈÅì‰∏çÂåπÈÖçÁöÑÈîôËØØ„ÄÇ")
    
    # ÂàõÂª∫Êñ∞Ê®°ÂûãÂÆû‰æã
    new_model = copy.deepcopy(model)
    
    # ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂåñÁâàÂÆûÁé∞ÔºåÈÄÇÁî®‰∫éÁÆÄÂçïÁΩëÁªú
    # ÂØπ‰∫éÂ§çÊùÇÁΩëÁªúÔºåÂ∫îËØ•‰ΩøÁî®ÂÉètorch_pruningËøôÊ†∑ÁöÑÂ∫ìÊù•‰øùÊåÅ‰æùËµñÂÖ≥Á≥ª‰∏ÄËá¥
    
    # È¶ñÂÖàËé∑ÂèñÊâÄÊúâConv2dÂ±ÇÁöÑÈõ∂ÈÄöÈÅìÊé©Á†Å
    zero_masks = {}
    for name, module in new_model.named_modules():
        if isinstance(module, nn.Conv2d):
            weight = module.weight.data
            out_channels = weight.shape[0]
            
            # ÊâæÂá∫Èõ∂ÈÄöÈÅì
            nonzero_mask = []
            for i in range(out_channels):
                channel_norm = torch.sum(torch.abs(weight[i]))
                nonzero_mask.append(channel_norm > 0)
            
            nonzero_mask = torch.tensor(nonzero_mask, dtype=torch.bool, device=device)
            nonzero_count = torch.sum(nonzero_mask).item()
            
            # Â¶ÇÊûúÊúâÈÄöÈÅìÂèØ‰ª•ÁßªÈô§
            if nonzero_count < out_channels and nonzero_count > 0:
                zero_masks[name] = (nonzero_mask, nonzero_count)
    
    # ÂàõÂª∫Âç∑ÁßØÂ±ÇÂêçÁß∞Âà∞ÂÖ∂ÂêéÁöÑBatchNormÂ±ÇÁöÑÊò†Â∞Ñ
    conv_to_bn = {}
    prev_name = None
    
    # ÈÄöËøáÊ®°ÂûãÁöÑÂëΩÂêçÊ®°ÂùóÊâæÂá∫Âç∑ÁßØÂ±Ç‰∏éÂÖ∂ÂêéÁöÑBatchNormÂ±Ç
    for name, module in new_model.named_modules():
        if isinstance(module, nn.BatchNorm2d) and prev_name is not None and prev_name in zero_masks:
            conv_to_bn[prev_name] = name
        
        if isinstance(module, nn.Conv2d):
            prev_name = name
        else:
            prev_name = None
    
    # ÈÅçÂéÜÂπ∂‰øÆÊîπÂç∑ÁßØÂ±ÇÂíåÁõ∏Â∫îÁöÑBatchNormÂ±Ç
    for conv_name, (nonzero_mask, nonzero_count) in zero_masks.items():
        # Ëé∑ÂèñÂç∑ÁßØÂ±Ç
        conv_path = conv_name.split('.')
        conv_module = new_model
        for part in conv_path:
            conv_module = getattr(conv_module, part)
        
        # ÂØπ‰∫éResNetÔºåÊàë‰ª¨Âè™Â§ÑÁêÜÊüê‰∫õÂÆâÂÖ®ÁöÑÂ±ÇÔºåË∑≥ËøáÂèØËÉΩÁ†¥ÂùèÁªìÊûÑÁöÑÂ±Ç
        if 'resnet' in str(type(model)).lower():
            # Ë∑≥Ëøá‰∏ãÈááÊ†∑Â±Ç(downsample)ÂíåÂèØËÉΩÂΩ±ÂìçÊÆãÂ∑ÆËøûÊé•ÁöÑÂ±Ç
            if 'downsample' in conv_name or conv_name.endswith('conv1') or conv_name.endswith('conv3'):
                logging.warning(f"‚ö†Ô∏è Ë∑≥ËøáResNetÂÖ≥ÈîÆÂ±Ç {conv_name} ‰ª•‰øùÊåÅÁΩëÁªúÁªìÊûÑ")
                continue
        
        logging.info(f"üîç Â±Ç {conv_name}: ÂèëÁé∞ {conv_module.out_channels - nonzero_count}/{conv_module.out_channels} ‰∏™Èõ∂ÈÄöÈÅì")
        
        # ÂàõÂª∫Êñ∞ÁöÑÂç∑ÁßØÂ±ÇÔºåËæìÂá∫ÈÄöÈÅìÂáèÂ∞ë
        try:
            # Ëé∑ÂèñÂèÇÊï∞
            in_channels = conv_module.in_channels
            out_channels = nonzero_count
            kernel_size = conv_module.kernel_size
            stride = conv_module.stride
            padding = conv_module.padding
            dilation = conv_module.dilation
            groups = conv_module.groups  # ‰øùÁïôÂéüÂßãÂàÜÁªÑ
            bias = conv_module.bias is not None
            
            # ÂàõÂª∫Êñ∞ÁöÑÂç∑ÁßØÂ±ÇÂπ∂ÁßªËá≥Áõ∏ÂêåËÆæÂ§á
            new_conv = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                dilation=dilation,
                groups=groups if groups==1 else max(1, out_channels//in_channels*in_channels),
                bias=bias
            ).to(device)  # ÁßªËá≥‰∏éÂéüÊ®°ÂûãÁõ∏ÂêåÁöÑËÆæÂ§á
            
            # Â§çÂà∂ÈùûÈõ∂ÈÄöÈÅìÁöÑÊùÉÈáç
            idx = 0
            for i in range(conv_module.out_channels):
                if nonzero_mask[i]:
                    new_conv.weight.data[idx] = conv_module.weight.data[i]
                    # È¢ùÂ§ñÊ£ÄÊü•Á°Æ‰øùbiasÂ≠òÂú®
                    if bias and conv_module.bias is not None and new_conv.bias is not None:
                        new_conv.bias.data[idx] = conv_module.bias.data[i]
                    idx += 1
            
            # ÊõøÊç¢Âç∑ÁßØÂ±Ç
            parent_name = '.'.join(conv_path[:-1])
            attr_name = conv_path[-1]
            
            if parent_name:
                parent = new_model
                for part in parent_name.split('.'):
                    parent = getattr(parent, part)
                setattr(parent, attr_name, new_conv)
            else:
                setattr(new_model, attr_name, new_conv)
            
            logging.info(f"‚úÖ ÊàêÂäüÊõøÊç¢Âç∑ÁßØÂ±Ç {conv_name}ÔºåËæìÂá∫ÈÄöÈÅì‰ªé {conv_module.out_channels} ÂáèÂ∞ëÂà∞ {nonzero_count}")
            
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÁõ∏Â∫îÁöÑBatchNormÂ±ÇÈúÄË¶ÅÊõ¥Êñ∞
            if conv_name in conv_to_bn:
                bn_name = conv_to_bn[conv_name]
                bn_path = bn_name.split('.')
                bn_module = new_model
                for part in bn_path:
                    bn_module = getattr(bn_module, part)
                
                # ÂàõÂª∫Êñ∞ÁöÑBatchNormÂ±Ç
                new_bn = nn.BatchNorm2d(
                    num_features=nonzero_count,
                    eps=bn_module.eps,
                    momentum=bn_module.momentum,
                    affine=bn_module.affine,
                    track_running_stats=bn_module.track_running_stats
                ).to(device)
                
                # Â§çÂà∂ÈùûÈõ∂ÈÄöÈÅìÁöÑÂèÇÊï∞
                if bn_module.affine:
                    idx = 0
                    for i in range(bn_module.num_features):
                        if nonzero_mask[i]:
                            new_bn.weight.data[idx] = bn_module.weight.data[i]
                            new_bn.bias.data[idx] = bn_module.bias.data[i]
                            idx += 1
                
                # Â§çÂà∂ËøêË°åÊó∂ÁªüËÆ°Êï∞ÊçÆ
                if bn_module.track_running_stats:
                    idx = 0
                    for i in range(bn_module.num_features):
                        if nonzero_mask[i]:
                            # Á°Æ‰øù‰∏çËÆøÈóÆNone
                            if hasattr(bn_module, 'running_mean') and bn_module.running_mean is not None and \
                               hasattr(new_bn, 'running_mean') and new_bn.running_mean is not None:
                                new_bn.running_mean[idx] = bn_module.running_mean[i]
                            
                            if hasattr(bn_module, 'running_var') and bn_module.running_var is not None and \
                               hasattr(new_bn, 'running_var') and new_bn.running_var is not None:
                                new_bn.running_var[idx] = bn_module.running_var[i]
                            
                            idx += 1
                
                # ÊõøÊç¢BatchNormÂ±Ç
                parent_name = '.'.join(bn_path[:-1])
                attr_name = bn_path[-1]
                
                if parent_name:
                    parent = new_model
                    for part in parent_name.split('.'):
                        parent = getattr(parent, part)
                    setattr(parent, attr_name, new_bn)
                else:
                    setattr(new_model, attr_name, new_bn)
                
                logging.info(f"‚úÖ ÊàêÂäüÊõøÊç¢BatchNormÂ±Ç {bn_name}ÔºåÁâπÂæÅÊï∞‰ªé {bn_module.num_features} ÂáèÂ∞ëÂà∞ {nonzero_count}")
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Â§ÑÁêÜÂ±Ç {conv_name} Êó∂Âá∫Èîô: {e}")
    
    # ËÆ°ÁÆóÂèÇÊï∞Êï∞Èáè‰ª•È™åËØÅÁªìÊûÑÊòØÂê¶ÁúüÁöÑÂèòÂ∞è
    orig_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    new_params = sum(p.numel() for p in new_model.parameters() if p.requires_grad)
    reduction = (orig_params - new_params) / orig_params * 100 if orig_params > 0 else 0
    
    logging.info(f"üìä ÂèÇÊï∞: {orig_params:,} ‚Üí {new_params:,} (ÂáèÂ∞ë‰∫Ü {reduction:.2f}%)")
    
    # Á°Æ‰øùÊï¥‰∏™Ê®°ÂûãÈÉΩÂú®Ê≠£Á°ÆÁöÑËÆæÂ§á‰∏ä
    new_model = new_model.to(device)
    
    # Â∞ùËØïËøêË°å‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠‰ª•È™åËØÅÊ®°ÂûãÁªìÊûÑ
    try:
        dummy_input = torch.randn(1, 3, 32, 32).to(device)  # Á§∫‰æãËæìÂÖ•ÔºåÊ†πÊçÆÊ®°ÂûãË∞ÉÊï¥Â∞∫ÂØ∏
        new_model.eval()
        with torch.no_grad():
            _ = new_model(dummy_input)
        logging.info("‚úÖ ÈáçÂª∫ÂêéÁöÑÊ®°ÂûãÈ™åËØÅÊàêÂäüÔºåÁªìÊûÑ‰∏ÄËá¥")
    except Exception as e:
        logging.error(f"‚ùå Ê®°ÂûãÁªìÊûÑ‰∏ç‰∏ÄËá¥ÔºåÂèØËÉΩÈúÄË¶Å‰∏ì‰∏öÁöÑÁªìÊûÑÂåñÂâ™ÊûùÂ∑•ÂÖ∑: {e}")
        # Áî±‰∫éÊàë‰ª¨‰∏çÂ∏åÊúõ‰∏≠Êñ≠ÊâßË°åÊµÅÁ®ãÔºåËøôÈáåËøîÂõûÂéüÂßãÊ®°Âûã
        logging.warning("‚ö†Ô∏è ËøîÂõûÂéüÂßãÊ®°ÂûãÔºàÂ∏¶Èõ∂ÊùÉÈáçÔºâ")
        return model
    
    return new_model

def prune_model_properly(model, pruning_ratio=0.5, dataset='cifar10'):
    try:
        import torch_pruning as tp
        
        logging.info("üî™ ÂºÄÂßãÁªìÊûÑÂåñÂâ™ÊûùÔºå‰ΩøÁî®torch_pruningÂ∫ì...")
        
        # Ëé∑ÂèñËÆæÂ§á‰ø°ÊÅØ
        device = next(model.parameters()).device
        
        # ÂàõÂª∫Á§∫‰æãËæìÂÖ•
        if dataset == 'imagenet':
            example_inputs = torch.randn(1, 3, 224, 224).to(device)
        else:
            example_inputs = torch.randn(1, 3, 32, 32).to(device)
            
        # ÂàõÂª∫Ââ™ÊûùÂô® - ‰ΩøÁî®2.xÁâàÊú¨API
        importance = tp.importance.MagnitudeImportance(p=1)  # L1ËåÉÊï∞
        
        # ‰øùÊä§ÂàÜÁ±ªÂ±Ç
        ignored_layers = []
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                ignored_layers.append(module)
                logging.info(f"üõ°Ô∏è ‰øùÊä§ÂàÜÁ±ªÂ±Ç {name}")
                
        # ÂàõÂª∫Ââ™ÊûùÂô®
        pruner = tp.pruner.MagnitudePruner(
            model,
            example_inputs, 
            importance,
            ch_sparsity=pruning_ratio,  # ÈÄöÈÅìÁ®ÄÁñèÂ∫¶
            ignored_layers=ignored_layers,  # ‰øùÊä§Â±Ç
        )
        
        # ÊâßË°åÂâ™Êûù
        pruner.step()
        
        # È™åËØÅÊ®°ÂûãÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú
        model.eval()
        with torch.no_grad():
            output = model(example_inputs)
            logging.info(f"‚úÖ Ââ™ÊûùÂêéÊ®°ÂûãËæìÂá∫ÂΩ¢Áä∂: {output.shape}")
            
        # ËÆ°ÁÆóÂèÇÊï∞Êï∞Èáè
        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logging.info(f"üìä Ââ™ÊûùÂêéÁöÑÂèÇÊï∞Êï∞Èáè: {param_count:,}")
        
        return model
        
    except Exception as e:
        logging.error(f"‚ùå ÁªìÊûÑÂåñÂâ™ÊûùÂ§±Ë¥•: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return model

if __name__ == "__main__":
    args = parse_args()
    results, model = run_experiment(args) 